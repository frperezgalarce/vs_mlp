{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.functional as f \n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "from sklearn import manifold\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, pred_labels, ax):\n",
    "    #fig = plt.figure(figsize = (10, 10));\n",
    "    #ax = fig.add_subplot(1, 1, 1);\n",
    "    cm = metrics.confusion_matrix(labels, pred_labels, normalize='pred');\n",
    "    cm = metrics.ConfusionMatrixDisplay(cm);\n",
    "    cm.plot(cmap = 'Blues', ax = ax)\n",
    "    cm.im_.colorbar.remove()\n",
    "\n",
    "\n",
    "def get_predictions(model, iterator, device):\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred, _, _ = model(x)\n",
    "\n",
    "            y_prob = f.softmax(y_pred, dim = -1)\n",
    "            top_pred = y_prob.argmax(1, keepdim = True)\n",
    "\n",
    "            images.append(x.cpu())\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "    probs = torch.cat(probs, dim = 0)\n",
    "\n",
    "    return images, labels, probs\n",
    "\n",
    "def regularization_method(params):\n",
    "    l1_regularization = 0\n",
    "    l2_regularization = 0\n",
    "    lambda1 = 0.001\n",
    "    lambda2 = 0.001\n",
    "    for param in params:\n",
    "        l1_regularization += torch.norm(param, 1)**2\n",
    "        l2_regularization += torch.norm(param, 2)**2\n",
    "    loss = loss + lambda1*l1_regularization + lambda2*l2_regularization\n",
    "    \n",
    "def plot_weights(weights, n_weights):\n",
    "\n",
    "    rows = int(np.sqrt(n_weights))\n",
    "    cols = int(np.sqrt(n_weights))\n",
    "\n",
    "    fig = plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    for i in range(rows*cols):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(weights[i].view(5, 10).cpu().numpy(), cmap = 'bone')\n",
    "        #plt.title(str(train_target[i]))\n",
    "        ax.axis('off')\n",
    "        \n",
    "def get_pca(data, data_test=None, n_components = 2):\n",
    "    pca = decomposition.PCA()\n",
    "    \n",
    "    pca.n_components = n_components\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    \n",
    "    if data_test is not None: \n",
    "        pca_data_test = pca.transform(data_test)\n",
    "        return pca_data, pca_data_test \n",
    "    \n",
    "    return pca_data\n",
    "\n",
    "\n",
    "def get_tsne(data, data_test = None, n_components = 2, n_curves = None):\n",
    "    if n_curves is not None:\n",
    "        data = data[:n_curves]\n",
    "    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    \n",
    "    if data_test is not None: \n",
    "        tsne_data_test = tsne.fit_transform(data_test)\n",
    "        return tsne_data, tsne_data_test  \n",
    "    \n",
    "    return tsne_data\n",
    "\n",
    "def get_representations(model, iterator, device):\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    intermediates = []\n",
    "    intermediates2 = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in iterator:\n",
    "            x = x.to(device)\n",
    "            y_pred, h2, h1 = net(x)\n",
    "            outputs.append(y_pred.cpu())\n",
    "            intermediates.append(h1.cpu())\n",
    "            intermediates2.append(h2.cpu())\n",
    "            labels.append(y)\n",
    "        \n",
    "    outputs = torch.cat(outputs, dim = 0)\n",
    "    intermediates = torch.cat(intermediates, dim = 0)\n",
    "    intermediates2 = torch.cat(intermediates2, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "\n",
    "    return outputs, intermediates, intermediates2, labels\n",
    "\n",
    "def plot_representations(data, labels, ax, n_curves = None):\n",
    "    if n_curves is not None:\n",
    "        data = data[:n_curves]\n",
    "        labels = labels[:n_curves]\n",
    "    #fig = plt.figure(figsize = (10, 10))\n",
    "    #ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, alpha =0.5)\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    legend = ax.legend(handles = handles, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 1\n",
    "fileTrain = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Train_rrlyr-'+str(number)+'.csv'\n",
    "fileTest = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Test_rrlyr-'+str(number)+'.csv'\n",
    "train_dataset = pd.read_csv(fileTrain, index_col ='Unnamed: 0')\n",
    "test_dataset = pd.read_csv(fileTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq1_harmonics_rel_phase_0\n",
      "Freq2_harmonics_rel_phase_0\n",
      "Freq3_harmonics_rel_phase_0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_dataset =  train_dataset.drop(['Pred', 'Pred2', 'h', 'e', 'u','ID'], axis = 1)\n",
    "    for col in train_dataset.columns:\n",
    "        if col not in ['label']:\n",
    "            if train_dataset[col].var()==0:\n",
    "                print(col)\n",
    "                del train_dataset[col]\n",
    "    test_dataset = test_dataset[list(train_dataset.columns)]\n",
    "except:\n",
    "    print(col)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100000#train_dataset.shape[0] \n",
    "epsilon = 0\n",
    "input_size = train_dataset.shape[1]-1\n",
    "hidden_size = 50\n",
    "hidden_size2 = 50\n",
    "num_classes = 2\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "regularization = False\n",
    "add_DR_based_data = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 61)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.sample(n)\n",
    "train_dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28625, 61)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset['PeriodLS'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78291, 61)\n"
     ]
    }
   ],
   "source": [
    "label = train_dataset['label']\n",
    "del train_dataset['label']\n",
    "\n",
    "\n",
    "train_dataset_z=(train_dataset-train_dataset.mean())/train_dataset.std()\n",
    "z_scores = stats.zscore(train_dataset_z)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "train_dataset['label'] = label\n",
    "train_dataset = train_dataset[filtered_entries]\n",
    "print(train_dataset.shape)\n",
    "train_dataset_pred = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28625, 60)\n",
      "(28625, 61)\n"
     ]
    }
   ],
   "source": [
    "label = test_dataset['label']\n",
    "del test_dataset['label']\n",
    "\n",
    "print(test_dataset.shape)\n",
    "\n",
    "test_dataset_z=(test_dataset-test_dataset.mean())/test_dataset.std()\n",
    "z_scores = stats.zscore(test_dataset_z)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "test_dataset['label'] = label\n",
    "\n",
    "print(test_dataset.shape)\n",
    "\n",
    "test_dataset = test_dataset[filtered_entries]\n",
    "\n",
    "test_dataset_pred = test_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Amplitude', 'AndersonDarling', 'Autocor_length', 'Beyond1Std',\n",
       "       'CAR_mean', 'CAR_sigma', 'CAR_tau', 'Con', 'Eta_e',\n",
       "       'FluxPercentileRatioMid20', 'FluxPercentileRatioMid35',\n",
       "       'FluxPercentileRatioMid50', 'FluxPercentileRatioMid65',\n",
       "       'FluxPercentileRatioMid80', 'Freq1_harmonics_amplitude_0',\n",
       "       'Freq1_harmonics_amplitude_1', 'Freq1_harmonics_amplitude_2',\n",
       "       'Freq1_harmonics_amplitude_3', 'Freq1_harmonics_rel_phase_1',\n",
       "       'Freq1_harmonics_rel_phase_2', 'Freq1_harmonics_rel_phase_3',\n",
       "       'Freq2_harmonics_amplitude_0', 'Freq2_harmonics_amplitude_1',\n",
       "       'Freq2_harmonics_amplitude_2', 'Freq2_harmonics_amplitude_3',\n",
       "       'Freq2_harmonics_rel_phase_1', 'Freq2_harmonics_rel_phase_2',\n",
       "       'Freq2_harmonics_rel_phase_3', 'Freq3_harmonics_amplitude_0',\n",
       "       'Freq3_harmonics_amplitude_1', 'Freq3_harmonics_amplitude_2',\n",
       "       'Freq3_harmonics_amplitude_3', 'Freq3_harmonics_rel_phase_1',\n",
       "       'Freq3_harmonics_rel_phase_2', 'Freq3_harmonics_rel_phase_3', 'Gskew',\n",
       "       'LinearTrend', 'MaxSlope', 'Mean', 'Meanvariance', 'MedianAbsDev',\n",
       "       'MedianBRP', 'PairSlopeTrend', 'PercentAmplitude',\n",
       "       'PercentDifferenceFluxPercentile', 'PeriodLS', 'Period_fit', 'Psi_CS',\n",
       "       'Psi_eta', 'Q31', 'Rcs', 'Skew', 'SlottedA_length', 'SmallKurtosis',\n",
       "       'Std', 'StetsonK', 'StetsonK_AC', 'StructureFunction_index_21',\n",
       "       'StructureFunction_index_31', 'StructureFunction_index_32', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>FluxPercentileRatioMid20</th>\n",
       "      <th>...</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>391262</th>\n",
       "      <td>0.15100</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>10</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>22.791959</td>\n",
       "      <td>0.537868</td>\n",
       "      <td>0.619429</td>\n",
       "      <td>0.02349</td>\n",
       "      <td>14.104876</td>\n",
       "      <td>0.145522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194838</td>\n",
       "      <td>17.79498</td>\n",
       "      <td>-0.488208</td>\n",
       "      <td>0.078843</td>\n",
       "      <td>0.814231</td>\n",
       "      <td>0.784707</td>\n",
       "      <td>1.801910</td>\n",
       "      <td>2.535099</td>\n",
       "      <td>1.448028</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230291</th>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.115532</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>274.744191</td>\n",
       "      <td>0.211135</td>\n",
       "      <td>0.053812</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.686240</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133719</td>\n",
       "      <td>5.69361</td>\n",
       "      <td>0.111186</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.793480</td>\n",
       "      <td>0.745731</td>\n",
       "      <td>1.620815</td>\n",
       "      <td>2.129265</td>\n",
       "      <td>1.607298</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26148</th>\n",
       "      <td>0.02575</td>\n",
       "      <td>0.536316</td>\n",
       "      <td>2</td>\n",
       "      <td>0.299065</td>\n",
       "      <td>39.534998</td>\n",
       "      <td>0.553698</td>\n",
       "      <td>0.389701</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6307.485427</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058798</td>\n",
       "      <td>4.92600</td>\n",
       "      <td>-0.091798</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.791180</td>\n",
       "      <td>0.738202</td>\n",
       "      <td>1.453116</td>\n",
       "      <td>1.593353</td>\n",
       "      <td>1.206530</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34003</th>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.253883</td>\n",
       "      <td>5</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>206.752429</td>\n",
       "      <td>0.049652</td>\n",
       "      <td>0.068576</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.364235</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502323</td>\n",
       "      <td>1.18029</td>\n",
       "      <td>1.457469</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>0.777735</td>\n",
       "      <td>0.741196</td>\n",
       "      <td>1.317766</td>\n",
       "      <td>1.621916</td>\n",
       "      <td>1.385394</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189282</th>\n",
       "      <td>0.45500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>18.179650</td>\n",
       "      <td>0.189635</td>\n",
       "      <td>1.087083</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.117858</td>\n",
       "      <td>0.088483</td>\n",
       "      <td>...</td>\n",
       "      <td>1.129390</td>\n",
       "      <td>1.89933</td>\n",
       "      <td>2.278362</td>\n",
       "      <td>0.219409</td>\n",
       "      <td>0.742077</td>\n",
       "      <td>0.726959</td>\n",
       "      <td>2.211178</td>\n",
       "      <td>3.532070</td>\n",
       "      <td>1.632040</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Amplitude  AndersonDarling  Autocor_length  Beyond1Std    CAR_mean  \\\n",
       "391262    0.15100         0.001250              10    0.346667   22.791959   \n",
       "230291    0.01750         0.115532               2    0.363333  274.744191   \n",
       "26148     0.02575         0.536316               2    0.299065   39.534998   \n",
       "34003     0.01825         0.253883               5    0.283186  206.752429   \n",
       "189282    0.45500         0.000000               1    0.220000   18.179650   \n",
       "\n",
       "        CAR_sigma   CAR_tau      Con        Eta_e  FluxPercentileRatioMid20  \\\n",
       "391262   0.537868  0.619429  0.02349    14.104876                  0.145522   \n",
       "230291   0.211135  0.053812  0.00000     5.686240                  0.166667   \n",
       "26148    0.553698  0.389701  0.00000  6307.485427                  0.186047   \n",
       "34003    0.049652  0.068576  0.00000     8.364235                  0.156250   \n",
       "189282   0.189635  1.087083  0.00000    17.117858                  0.088483   \n",
       "\n",
       "        ...      Skew  SlottedA_length  SmallKurtosis       Std  StetsonK  \\\n",
       "391262  ...  0.194838         17.79498      -0.488208  0.078843  0.814231   \n",
       "230291  ...  0.133719          5.69361       0.111186  0.009347  0.793480   \n",
       "26148   ... -0.058798          4.92600      -0.091798  0.012686  0.791180   \n",
       "34003   ... -0.502323          1.18029       1.457469  0.010113  0.777735   \n",
       "189282  ...  1.129390          1.89933       2.278362  0.219409  0.742077   \n",
       "\n",
       "        StetsonK_AC  StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "391262     0.784707                    1.801910                    2.535099   \n",
       "230291     0.745731                    1.620815                    2.129265   \n",
       "26148      0.738202                    1.453116                    1.593353   \n",
       "34003      0.741196                    1.317766                    1.621916   \n",
       "189282     0.726959                    2.211178                    3.532070   \n",
       "\n",
       "        StructureFunction_index_32   label  \n",
       "391262                    1.448028  ClassB  \n",
       "230291                    1.607298  ClassB  \n",
       "26148                     1.206530  ClassB  \n",
       "34003                     1.385394  ClassB  \n",
       "189282                    1.632040  ClassB  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 3000\n",
    "samples1 = samples*2\n",
    "number_columns = train_dataset.shape[1]\n",
    "option = 2\n",
    "\n",
    "\n",
    "data_prior = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns)\n",
    "\n",
    "if add_DR_based_data:\n",
    "    #option 1\n",
    "    if option == 1:\n",
    "        for i in range(samples1):\n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']= (np.random.uniform(0.2-epsilon,1.0+epsilon))#-minimum_period)/(maximum_period-minimum_period)\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    if option==2:\n",
    "        #option 2\n",
    "        for i in range(samples):\n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']=(np.random.uniform(0.1,0.2))#-minimum_period)/(maximum_period-minimum_period)\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']=(np.random.uniform(1.0,1.1))\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        \n",
    "    #option 3\n",
    "    if option==3:\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) #pd.DataFrame([train_dataset.sample(1000).mean()]).T\n",
    "            new_data['PeriodLS']= 1.0\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']= 0.2\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amplitude    0.221300\n",
      "PeriodLS     1.375133\n",
      "dtype: float64\n",
      "           Amplitude  PeriodLS\n",
      "Amplitude   0.006363  0.014775\n",
      "PeriodLS    0.014775  0.064467\n",
      "Amplitude    0.204928\n",
      "PeriodLS     0.277649\n",
      "dtype: float64\n",
      "           Amplitude  PeriodLS\n",
      "Amplitude   0.001840  0.000139\n",
      "PeriodLS    0.000139  0.000379\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>PeriodLS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.246106</td>\n",
       "      <td>0.290368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.211656</td>\n",
       "      <td>0.262357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.123786</td>\n",
       "      <td>0.255924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.164427</td>\n",
       "      <td>0.258731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.258078</td>\n",
       "      <td>0.304855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.226020</td>\n",
       "      <td>0.314029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.130954</td>\n",
       "      <td>0.283266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.204666</td>\n",
       "      <td>0.327309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.199067</td>\n",
       "      <td>0.279028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.215762</td>\n",
       "      <td>0.286214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.240055</td>\n",
       "      <td>0.264379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.209038</td>\n",
       "      <td>0.281678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.216306</td>\n",
       "      <td>0.247204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.300985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.259850</td>\n",
       "      <td>0.273436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.184198</td>\n",
       "      <td>0.258145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.247591</td>\n",
       "      <td>0.288643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.187887</td>\n",
       "      <td>0.278494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.224612</td>\n",
       "      <td>0.259512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.204014</td>\n",
       "      <td>0.297229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.239735</td>\n",
       "      <td>0.299390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.297919</td>\n",
       "      <td>0.306207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.198300</td>\n",
       "      <td>0.287097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.207285</td>\n",
       "      <td>0.309259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.217800</td>\n",
       "      <td>0.320225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.183727</td>\n",
       "      <td>0.309077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.206249</td>\n",
       "      <td>0.277650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.262207</td>\n",
       "      <td>0.274792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.258267</td>\n",
       "      <td>0.275131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.169979</td>\n",
       "      <td>0.225364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4970</th>\n",
       "      <td>0.194768</td>\n",
       "      <td>0.256575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4971</th>\n",
       "      <td>0.239863</td>\n",
       "      <td>0.256631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4972</th>\n",
       "      <td>0.213847</td>\n",
       "      <td>0.297269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4973</th>\n",
       "      <td>0.175435</td>\n",
       "      <td>0.289780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4974</th>\n",
       "      <td>0.199353</td>\n",
       "      <td>0.264955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4975</th>\n",
       "      <td>0.188662</td>\n",
       "      <td>0.262287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4976</th>\n",
       "      <td>0.240255</td>\n",
       "      <td>0.272283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4977</th>\n",
       "      <td>0.079787</td>\n",
       "      <td>0.288262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4978</th>\n",
       "      <td>0.180731</td>\n",
       "      <td>0.276630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4979</th>\n",
       "      <td>0.174567</td>\n",
       "      <td>0.306611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4980</th>\n",
       "      <td>0.197335</td>\n",
       "      <td>0.248259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>0.154406</td>\n",
       "      <td>0.268404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4982</th>\n",
       "      <td>0.266690</td>\n",
       "      <td>0.288557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4983</th>\n",
       "      <td>0.173598</td>\n",
       "      <td>0.269478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4984</th>\n",
       "      <td>0.278874</td>\n",
       "      <td>0.270814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4985</th>\n",
       "      <td>0.225183</td>\n",
       "      <td>0.302856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4986</th>\n",
       "      <td>0.235693</td>\n",
       "      <td>0.294189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4987</th>\n",
       "      <td>0.245715</td>\n",
       "      <td>0.253994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4988</th>\n",
       "      <td>0.197515</td>\n",
       "      <td>0.262570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4989</th>\n",
       "      <td>0.218419</td>\n",
       "      <td>0.282745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4990</th>\n",
       "      <td>0.236424</td>\n",
       "      <td>0.279494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4991</th>\n",
       "      <td>0.160310</td>\n",
       "      <td>0.310802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4992</th>\n",
       "      <td>0.223270</td>\n",
       "      <td>0.257392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4993</th>\n",
       "      <td>0.181535</td>\n",
       "      <td>0.278877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4994</th>\n",
       "      <td>0.199034</td>\n",
       "      <td>0.276766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>0.159723</td>\n",
       "      <td>0.265558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.238345</td>\n",
       "      <td>0.278039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.098086</td>\n",
       "      <td>0.280852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.197220</td>\n",
       "      <td>0.268936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.186412</td>\n",
       "      <td>0.267278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Amplitude  PeriodLS\n",
       "0      0.246106  0.290368\n",
       "1      0.211656  0.262357\n",
       "2      0.123786  0.255924\n",
       "3      0.164427  0.258731\n",
       "4      0.258078  0.304855\n",
       "5      0.226020  0.314029\n",
       "6      0.130954  0.283266\n",
       "7      0.204666  0.327309\n",
       "8      0.199067  0.279028\n",
       "9      0.215762  0.286214\n",
       "10     0.240055  0.264379\n",
       "11     0.209038  0.281678\n",
       "12     0.216306  0.247204\n",
       "13     0.146900  0.300985\n",
       "14     0.259850  0.273436\n",
       "15     0.184198  0.258145\n",
       "16     0.247591  0.288643\n",
       "17     0.187887  0.278494\n",
       "18     0.224612  0.259512\n",
       "19     0.204014  0.297229\n",
       "20     0.239735  0.299390\n",
       "21     0.297919  0.306207\n",
       "22     0.198300  0.287097\n",
       "23     0.207285  0.309259\n",
       "24     0.217800  0.320225\n",
       "25     0.183727  0.309077\n",
       "26     0.206249  0.277650\n",
       "27     0.262207  0.274792\n",
       "28     0.258267  0.275131\n",
       "29     0.169979  0.225364\n",
       "...         ...       ...\n",
       "4970   0.194768  0.256575\n",
       "4971   0.239863  0.256631\n",
       "4972   0.213847  0.297269\n",
       "4973   0.175435  0.289780\n",
       "4974   0.199353  0.264955\n",
       "4975   0.188662  0.262287\n",
       "4976   0.240255  0.272283\n",
       "4977   0.079787  0.288262\n",
       "4978   0.180731  0.276630\n",
       "4979   0.174567  0.306611\n",
       "4980   0.197335  0.248259\n",
       "4981   0.154406  0.268404\n",
       "4982   0.266690  0.288557\n",
       "4983   0.173598  0.269478\n",
       "4984   0.278874  0.270814\n",
       "4985   0.225183  0.302856\n",
       "4986   0.235693  0.294189\n",
       "4987   0.245715  0.253994\n",
       "4988   0.197515  0.262570\n",
       "4989   0.218419  0.282745\n",
       "4990   0.236424  0.279494\n",
       "4991   0.160310  0.310802\n",
       "4992   0.223270  0.257392\n",
       "4993   0.181535  0.278877\n",
       "4994   0.199034  0.276766\n",
       "4995   0.159723  0.265558\n",
       "4996   0.238345  0.278039\n",
       "4997   0.098086  0.280852\n",
       "4998   0.197220  0.268936\n",
       "4999   0.186412  0.267278\n",
       "\n",
       "[5000 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class_filtered_upper = train_dataset[(train_dataset.label=='ClassA') & (train_dataset.PeriodLS>0.9)]\n",
    "class_filtered_lower = train_dataset[(train_dataset.label=='ClassA') & (train_dataset.PeriodLS<0.3)]\n",
    "\n",
    "mean_upper = (class_filtered_upper[['Amplitude', 'PeriodLS']].mean())\n",
    "cov_upper =  (class_filtered_upper[['Amplitude', 'PeriodLS']].cov())\n",
    "\n",
    "samples_upper = pd.DataFrame(np.random.multivariate_normal(mean_upper, cov_upper, 5000), columns=['Amplitude', 'PeriodLS'])\n",
    "\n",
    "\n",
    "mean_lower = (class_filtered_lower[['Amplitude', 'PeriodLS']].mean())\n",
    "cov_lower =  (class_filtered_lower[['Amplitude', 'PeriodLS']].cov())\n",
    "\n",
    "samples_lower = pd.DataFrame(np.random.multivariate_normal(mean_lower, cov_lower, 5000), columns=['Amplitude', 'PeriodLS'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>FluxPercentileRatioMid20</th>\n",
       "      <th>...</th>\n",
       "      <th>Rcs</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "      <td>6001.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Amplitude  AndersonDarling  Autocor_length  Beyond1Std  CAR_mean  \\\n",
       "count     6001.0           6001.0          6001.0      6001.0    6001.0   \n",
       "mean         0.0              0.0             0.0         0.0       0.0   \n",
       "std          0.0              0.0             0.0         0.0       0.0   \n",
       "min          0.0              0.0             0.0         0.0       0.0   \n",
       "25%          0.0              0.0             0.0         0.0       0.0   \n",
       "50%          0.0              0.0             0.0         0.0       0.0   \n",
       "75%          0.0              0.0             0.0         0.0       0.0   \n",
       "max          0.0              0.0             0.0         0.0       0.0   \n",
       "\n",
       "       CAR_sigma  CAR_tau     Con   Eta_e  FluxPercentileRatioMid20  ...  \\\n",
       "count     6001.0   6001.0  6001.0  6001.0                    6001.0  ...   \n",
       "mean         0.0      0.0     0.0     0.0                       0.0  ...   \n",
       "std          0.0      0.0     0.0     0.0                       0.0  ...   \n",
       "min          0.0      0.0     0.0     0.0                       0.0  ...   \n",
       "25%          0.0      0.0     0.0     0.0                       0.0  ...   \n",
       "50%          0.0      0.0     0.0     0.0                       0.0  ...   \n",
       "75%          0.0      0.0     0.0     0.0                       0.0  ...   \n",
       "max          0.0      0.0     0.0     0.0                       0.0  ...   \n",
       "\n",
       "          Rcs    Skew  SlottedA_length  SmallKurtosis     Std  StetsonK  \\\n",
       "count  6001.0  6001.0           6001.0         6001.0  6001.0    6001.0   \n",
       "mean      0.0     0.0              0.0            0.0     0.0       0.0   \n",
       "std       0.0     0.0              0.0            0.0     0.0       0.0   \n",
       "min       0.0     0.0              0.0            0.0     0.0       0.0   \n",
       "25%       0.0     0.0              0.0            0.0     0.0       0.0   \n",
       "50%       0.0     0.0              0.0            0.0     0.0       0.0   \n",
       "75%       0.0     0.0              0.0            0.0     0.0       0.0   \n",
       "max       0.0     0.0              0.0            0.0     0.0       0.0   \n",
       "\n",
       "       StetsonK_AC  StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "count       6001.0                      6001.0                      6001.0   \n",
       "mean           0.0                         0.0                         0.0   \n",
       "std            0.0                         0.0                         0.0   \n",
       "min            0.0                         0.0                         0.0   \n",
       "25%            0.0                         0.0                         0.0   \n",
       "50%            0.0                         0.0                         0.0   \n",
       "75%            0.0                         0.0                         0.0   \n",
       "max            0.0                         0.0                         0.0   \n",
       "\n",
       "       StructureFunction_index_32  \n",
       "count                      6001.0  \n",
       "mean                          0.0  \n",
       "std                           0.0  \n",
       "min                           0.0  \n",
       "25%                           0.0  \n",
       "50%                           0.0  \n",
       "75%                           0.0  \n",
       "max                           0.0  \n",
       "\n",
       "[8 rows x 60 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_prior.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior, val_dataset_prior = train_test_split(data_prior, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "\n",
    "train_target_prior = torch.tensor(train_dataset_prior['label'].values.astype(np.float32))\n",
    "train_prior = torch.tensor(train_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_prior = f.normalize(train_prior)\n",
    "train_tensor_prior = data_utils.TensorDataset(train_prior, train_target_prior) \n",
    "train_loader_prior = data_utils.DataLoader(dataset = train_tensor_prior, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>FluxPercentileRatioMid20</th>\n",
       "      <th>...</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5868</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2157</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1161</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3574</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Amplitude  AndersonDarling  Autocor_length  Beyond1Std  CAR_mean  \\\n",
       "2602          0                0               0           0         0   \n",
       "5868          0                0               0           0         0   \n",
       "2157          0                0               0           0         0   \n",
       "1161          0                0               0           0         0   \n",
       "3574          0                0               0           0         0   \n",
       "\n",
       "      CAR_sigma  CAR_tau  Con  Eta_e  FluxPercentileRatioMid20  ...  Skew  \\\n",
       "2602          0        0    0      0                         0  ...     0   \n",
       "5868          0        0    0      0                         0  ...     0   \n",
       "2157          0        0    0      0                         0  ...     0   \n",
       "1161          0        0    0      0                         0  ...     0   \n",
       "3574          0        0    0      0                         0  ...     0   \n",
       "\n",
       "      SlottedA_length  SmallKurtosis  Std  StetsonK  StetsonK_AC  \\\n",
       "2602                0              0    0         0            0   \n",
       "5868                0              0    0         0            0   \n",
       "2157                0              0    0         0            0   \n",
       "1161                0              0    0         0            0   \n",
       "3574                0              0    0         0            0   \n",
       "\n",
       "      StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "2602                           0                           0   \n",
       "5868                           0                           0   \n",
       "2157                           0                           0   \n",
       "1161                           0                           0   \n",
       "3574                           0                           0   \n",
       "\n",
       "      StructureFunction_index_32  label  \n",
       "2602                           0    0.5  \n",
       "5868                           0    0.5  \n",
       "2157                           0    0.5  \n",
       "1161                           0    0.5  \n",
       "3574                           0    0.5  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "val_target_prior = torch.tensor(val_dataset_prior['label'].values.astype(np.float32))\n",
    "val_prior = torch.tensor(val_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val_prior = f.normalize(val_prior)\n",
    "val_tensor_prior = data_utils.TensorDataset(val_prior, val_target_prior) \n",
    "val_loader_prior = data_utils.DataLoader(dataset = val_tensor_prior, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases using DR 1: Period $ \\in [0.2,1.0]$ days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassA', '1')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassB', '0')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('Noise', '0.5')\n",
    "train_target = torch.tensor(train_dataset['label'].values.astype(np.float32))\n",
    "train = torch.tensor(train_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train = f.normalize(train)\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "train_target_pred = torch.tensor(train_dataset_pred['label'].values.astype(np.float32))\n",
    "train_pred = torch.tensor(train_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_pred = f.normalize(train_pred)\n",
    "train_tensor_pred = data_utils.TensorDataset(train_pred, train_target_pred) \n",
    "train_loader_pred = data_utils.DataLoader(dataset = train_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassA', '1')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassB', '0')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('Noise', '0.5')\n",
    "val_target = torch.tensor(val_dataset['label'].values.astype(np.float32))\n",
    "val = torch.tensor(val_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val = f.normalize(val)\n",
    "val_tensor = data_utils.TensorDataset(val, val_target) \n",
    "val_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassA', '1')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassB', '0')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('Noise', '0.5')\n",
    "test_target = torch.tensor(test_dataset['label'].values.astype(np.float32))\n",
    "test = torch.tensor(test_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test = f.normalize(test)\n",
    "test_tensor = data_utils.TensorDataset(test, test_target) \n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "test_target_pred = torch.tensor(test_dataset_pred['label'].values.astype(np.float32))\n",
    "test_pred = torch.tensor(test_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test_pred = f.normalize(test_pred)\n",
    "test_tensor_pred = data_utils.TensorDataset(test_pred, test_target_pred) \n",
    "test_loader_pred = data_utils.DataLoader(dataset = test_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>FluxPercentileRatioMid20</th>\n",
       "      <th>...</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>244649</th>\n",
       "      <td>0.0135</td>\n",
       "      <td>0.092156</td>\n",
       "      <td>2</td>\n",
       "      <td>0.306122</td>\n",
       "      <td>17.688622</td>\n",
       "      <td>0.408969</td>\n",
       "      <td>0.858897</td>\n",
       "      <td>0.006897</td>\n",
       "      <td>137.587080</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>...</td>\n",
       "      <td>0.188347</td>\n",
       "      <td>3.15112</td>\n",
       "      <td>-0.287186</td>\n",
       "      <td>0.007020</td>\n",
       "      <td>0.811980</td>\n",
       "      <td>0.727058</td>\n",
       "      <td>1.438438</td>\n",
       "      <td>1.754671</td>\n",
       "      <td>1.410488</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172713</th>\n",
       "      <td>0.3940</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.401338</td>\n",
       "      <td>37.626600</td>\n",
       "      <td>0.449155</td>\n",
       "      <td>0.509367</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>808.470445</td>\n",
       "      <td>0.192635</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.541800</td>\n",
       "      <td>0.09454</td>\n",
       "      <td>-0.596343</td>\n",
       "      <td>0.218434</td>\n",
       "      <td>0.821823</td>\n",
       "      <td>0.839783</td>\n",
       "      <td>1.576597</td>\n",
       "      <td>1.939417</td>\n",
       "      <td>1.318366</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415332</th>\n",
       "      <td>0.0470</td>\n",
       "      <td>0.028426</td>\n",
       "      <td>3</td>\n",
       "      <td>0.330189</td>\n",
       "      <td>0.662727</td>\n",
       "      <td>0.008413</td>\n",
       "      <td>19.759952</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>4.637149</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.362621</td>\n",
       "      <td>5.25720</td>\n",
       "      <td>-0.533602</td>\n",
       "      <td>0.025552</td>\n",
       "      <td>0.845216</td>\n",
       "      <td>0.802270</td>\n",
       "      <td>1.577230</td>\n",
       "      <td>2.059057</td>\n",
       "      <td>1.354458</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8976</th>\n",
       "      <td>0.1605</td>\n",
       "      <td>0.001820</td>\n",
       "      <td>7</td>\n",
       "      <td>0.390000</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.014103</td>\n",
       "      <td>2814.322639</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>1.224533</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016928</td>\n",
       "      <td>18.82826</td>\n",
       "      <td>-0.732343</td>\n",
       "      <td>0.093597</td>\n",
       "      <td>0.831834</td>\n",
       "      <td>0.869900</td>\n",
       "      <td>2.014249</td>\n",
       "      <td>3.051537</td>\n",
       "      <td>1.529375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385758</th>\n",
       "      <td>0.0220</td>\n",
       "      <td>0.325322</td>\n",
       "      <td>2</td>\n",
       "      <td>0.293333</td>\n",
       "      <td>60.151715</td>\n",
       "      <td>0.459681</td>\n",
       "      <td>0.241105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1808.128505</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006312</td>\n",
       "      <td>0.04619</td>\n",
       "      <td>0.565611</td>\n",
       "      <td>0.011245</td>\n",
       "      <td>0.787133</td>\n",
       "      <td>0.675171</td>\n",
       "      <td>1.636484</td>\n",
       "      <td>2.158233</td>\n",
       "      <td>1.379109</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Amplitude  AndersonDarling  Autocor_length  Beyond1Std   CAR_mean  \\\n",
       "244649     0.0135         0.092156               2    0.306122  17.688622   \n",
       "172713     0.3940         0.000000               1    0.401338  37.626600   \n",
       "415332     0.0470         0.028426               3    0.330189   0.662727   \n",
       "8976       0.1605         0.001820               7    0.390000   0.004904   \n",
       "385758     0.0220         0.325322               2    0.293333  60.151715   \n",
       "\n",
       "        CAR_sigma      CAR_tau       Con        Eta_e  \\\n",
       "244649   0.408969     0.858897  0.006897   137.587080   \n",
       "172713   0.449155     0.509367  0.000000   808.470445   \n",
       "415332   0.008413    19.759952  0.009615     4.637149   \n",
       "8976     0.014103  2814.322639  0.003356     1.224533   \n",
       "385758   0.459681     0.241105  0.000000  1808.128505   \n",
       "\n",
       "        FluxPercentileRatioMid20  ...      Skew  SlottedA_length  \\\n",
       "244649                  0.173913  ...  0.188347          3.15112   \n",
       "172713                  0.192635  ... -0.541800          0.09454   \n",
       "415332                  0.215190  ... -0.362621          5.25720   \n",
       "8976                    0.176471  ...  0.016928         18.82826   \n",
       "385758                  0.142857  ...  0.006312          0.04619   \n",
       "\n",
       "        SmallKurtosis       Std  StetsonK  StetsonK_AC  \\\n",
       "244649      -0.287186  0.007020  0.811980     0.727058   \n",
       "172713      -0.596343  0.218434  0.821823     0.839783   \n",
       "415332      -0.533602  0.025552  0.845216     0.802270   \n",
       "8976        -0.732343  0.093597  0.831834     0.869900   \n",
       "385758       0.565611  0.011245  0.787133     0.675171   \n",
       "\n",
       "        StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "244649                    1.438438                    1.754671   \n",
       "172713                    1.576597                    1.939417   \n",
       "415332                    1.577230                    2.059057   \n",
       "8976                      2.014249                    3.051537   \n",
       "385758                    1.636484                    2.158233   \n",
       "\n",
       "        StructureFunction_index_32  label  \n",
       "244649                    1.410488      0  \n",
       "172713                    1.318366      1  \n",
       "415332                    1.354458      0  \n",
       "8976                      1.529375      0  \n",
       "385758                    1.379109      0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        #self.relu = nn.ReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size2)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes) \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.fc1(x)\n",
    "        #out = self.batchnorm1(out)\n",
    "        #out = self.relu(out)\n",
    "        out = self.relu(out1)\n",
    "        out2 = self.fc2(out)\n",
    "        #out = self.batchnorm1(out)\n",
    "        out = self.relu2(out)\n",
    "        out3 = self.fc3(out)\n",
    "        #x = self.dropout(x)\n",
    "        #out = self.sigmoid(out)\n",
    "        return out3, out2, out1\n",
    "  \n",
    "\n",
    "net_prior = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "net_prior.cuda()\n",
    "\n",
    "net = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "\n",
    "#net.cuda()\n",
    "net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net_prior.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, labels, weigths=None, weigths_prior=None):\n",
    "    regularization_loss = 0\n",
    "    lambda_1 = 0.00001\n",
    "    if weigths is not None: \n",
    "        regularization_loss += fast_cdist(weigths, weigths_prior)\n",
    "            #print(regularization_loss)\n",
    "        loss = criterion(outputs, labels) + lambda_1*regularization_loss #nn.L1Loss()(weigths, weigths)\n",
    "    else:\n",
    "        loss = criterion(outputs, labels)\n",
    "        #print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_cdist(x1, x2):\n",
    "    res=f.mse_loss(x1, x2, size_average=False)\n",
    "    #res=f.l1_loss(x1, x2, size_average=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Training----------------------------\n",
      "1 10.195698328977507\n",
      "-----------Validation----------------------------\n",
      "1 1.7103419680344432\n",
      "-----------Training----------------------------\n",
      "2 0.7694828326885517\n",
      "-----------Validation----------------------------\n",
      "2 0.3344830964740954\n",
      "-----------Training----------------------------\n",
      "3 0.21883059958734455\n",
      "-----------Validation----------------------------\n",
      "3 0.1391643348493074\n",
      "-----------Training----------------------------\n",
      "4 0.09670445622777092\n",
      "-----------Validation----------------------------\n",
      "4 0.0453172106491892\n",
      "-----------Training----------------------------\n",
      "5 0.023816462099199465\n",
      "-----------Validation----------------------------\n",
      "5 0.011832701532464279\n",
      "-----------Training----------------------------\n",
      "6 0.008116128176627074\n",
      "-----------Validation----------------------------\n",
      "6 0.005324338611803557\n",
      "-----------Training----------------------------\n",
      "7 0.004127825505634737\n",
      "-----------Validation----------------------------\n",
      "7 0.0030317055551629317\n",
      "-----------Training----------------------------\n",
      "8 0.002510728215324808\n",
      "-----------Validation----------------------------\n",
      "8 0.001960804587916324\n",
      "-----------Training----------------------------\n",
      "9 0.0016877383170043224\n",
      "-----------Validation----------------------------\n",
      "9 0.0013574800993266859\n",
      "-----------Training----------------------------\n",
      "10 0.0012095891512357271\n",
      "-----------Validation----------------------------\n",
      "10 0.000995485406172903\n",
      "-----------Training----------------------------\n",
      "11 0.000905818487765521\n",
      "-----------Validation----------------------------\n",
      "11 0.0007541556107370477\n",
      "-----------Training----------------------------\n",
      "12 0.0007006684703939765\n",
      "-----------Validation----------------------------\n",
      "12 0.0005882413763748972\n",
      "-----------Training----------------------------\n",
      "13 0.0005556083995209643\n",
      "-----------Validation----------------------------\n",
      "13 0.00046757647865696957\n",
      "-----------Training----------------------------\n",
      "14 0.0004492567841117904\n",
      "-----------Validation----------------------------\n",
      "14 0.00037707780536852385\n",
      "-----------Training----------------------------\n",
      "15 0.0003684893162292842\n",
      "-----------Validation----------------------------\n",
      "15 0.00031674535650956005\n",
      "-----------Training----------------------------\n",
      "16 0.00030731308389697555\n",
      "-----------Validation----------------------------\n",
      "16 0.0002564129076505962\n",
      "-----------Training----------------------------\n",
      "17 0.0002574991192338029\n",
      "-----------Validation----------------------------\n",
      "17 0.0002262466832211143\n",
      "-----------Training----------------------------\n",
      "18 0.00021898111648108127\n",
      "-----------Validation----------------------------\n",
      "18 0.00018099734657689144\n",
      "-----------Training----------------------------\n",
      "19 0.00018650963461610694\n",
      "-----------Validation----------------------------\n",
      "19 0.0001659142343621505\n",
      "-----------Training----------------------------\n",
      "20 0.00016004658309665658\n",
      "-----------Validation----------------------------\n",
      "20 0.00013574800993266858\n",
      "-----------Training----------------------------\n",
      "21 0.0001388894030328333\n",
      "-----------Validation----------------------------\n",
      "21 0.00012066489771792763\n",
      "-----------Training----------------------------\n",
      "22 0.00012115190720417091\n",
      "-----------Validation----------------------------\n",
      "22 0.00010558178550318668\n",
      "-----------Training----------------------------\n",
      "23 0.00010546988989474505\n",
      "-----------Validation----------------------------\n",
      "23 9.049867328844572e-05\n",
      "-----------Training----------------------------\n",
      "24 9.246549662753675e-05\n",
      "-----------Validation----------------------------\n",
      "24 7.541556107370477e-05\n",
      "-----------Training----------------------------\n",
      "25 8.355936355139377e-05\n",
      "-----------Validation----------------------------\n",
      "25 7.541556107370477e-05\n",
      "-----------Training----------------------------\n",
      "26 6.911035120134523e-05\n",
      "-----------Validation----------------------------\n",
      "26 6.0332448858963816e-05\n",
      "-----------Training----------------------------\n",
      "27 6.74103844095264e-05\n",
      "-----------Validation----------------------------\n",
      "27 4.524933664422286e-05\n",
      "-----------Training----------------------------\n",
      "28 5.177068992479313e-05\n",
      "-----------Validation----------------------------\n",
      "28 4.524933664422286e-05\n",
      "-----------Training----------------------------\n",
      "29 5.1412356675729243e-05\n",
      "-----------Validation----------------------------\n",
      "29 4.524933664422286e-05\n",
      "-----------Training----------------------------\n",
      "30 4.6208059999364366e-05\n",
      "-----------Validation----------------------------\n",
      "30 3.0166224429481908e-05\n",
      "-----------Training----------------------------\n",
      "31 3.552013600366356e-05\n",
      "-----------Validation----------------------------\n",
      "31 3.0166224429481908e-05\n",
      "-----------Training----------------------------\n",
      "32 3.523798383904632e-05\n",
      "-----------Validation----------------------------\n",
      "32 3.0166224429481908e-05\n",
      "-----------Training----------------------------\n",
      "33 3.496006395689835e-05\n",
      "-----------Validation----------------------------\n",
      "33 3.0166224429481908e-05\n",
      "-----------Training----------------------------\n",
      "34 3.29045854376618e-05\n",
      "-----------Validation----------------------------\n",
      "34 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "35 1.9218794692902875e-05\n",
      "-----------Validation----------------------------\n",
      "35 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "36 1.9021288177670813e-05\n",
      "-----------Validation----------------------------\n",
      "36 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "37 1.882660318408492e-05\n",
      "-----------Validation----------------------------\n",
      "37 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "38 1.8633328951322117e-05\n",
      "-----------Validation----------------------------\n",
      "38 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "39 1.844428700102857e-05\n",
      "-----------Validation----------------------------\n",
      "39 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "40 1.8258066572381194e-05\n",
      "-----------Validation----------------------------\n",
      "40 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "41 1.807607842620308e-05\n",
      "-----------Validation----------------------------\n",
      "41 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "42 1.7896911801671136e-05\n",
      "-----------Validation----------------------------\n",
      "42 1.5083112214740954e-05\n",
      "-----------Training----------------------------\n",
      "43 1.0161710208689672e-05\n",
      "-----------Validation----------------------------\n",
      "43 0.0\n",
      "-----------Training----------------------------\n",
      "44 2.3954718776003144e-06\n",
      "-----------Validation----------------------------\n",
      "44 0.0\n",
      "-----------Training----------------------------\n",
      "45 2.3150585106844028e-06\n",
      "-----------Validation----------------------------\n",
      "45 0.0\n",
      "-----------Training----------------------------\n",
      "46 2.2360559045915773e-06\n",
      "-----------Validation----------------------------\n",
      "46 0.0\n",
      "-----------Training----------------------------\n",
      "47 2.1556425376756657e-06\n",
      "-----------Validation----------------------------\n",
      "47 0.0\n",
      "-----------Training----------------------------\n",
      "48 2.0766399315828403e-06\n",
      "-----------Validation----------------------------\n",
      "48 0.0\n",
      "-----------Training----------------------------\n",
      "49 1.997637325490015e-06\n",
      "-----------Validation----------------------------\n",
      "49 0.0\n",
      "-----------Training----------------------------\n",
      "50 1.9186347193971894e-06\n",
      "-----------Validation----------------------------\n",
      "50 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the Model   \n",
    "hist_train = []\n",
    "hist_val = []\n",
    "num_epochs_prior = 50\n",
    "for epoch in range(num_epochs_prior):\n",
    "    \n",
    "    print('-----------Training----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for i, (star, labels) in enumerate(train_loader_prior):\n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs, _, _ = net_prior(star)\n",
    "        #print(softmax(outputs))\n",
    "        #m = nn.Sigmoid()\n",
    "        loss = criterion(outputs, labels.long())      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += outputs.shape[0] * loss.item()      \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    hist_train.append(epoch_loss)\n",
    "    print(epoch+1, epoch_loss / len(train_loader_prior))\n",
    "    \n",
    "    \n",
    "    print('-----------Validation----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (star, labels) in enumerate(val_loader_prior):  \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs, _, _ = net_prior(star)\n",
    "        loss = criterion(outputs, labels.long())    \n",
    "        epoch_loss += outputs.shape[0] * loss.item()      \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    print(epoch+1, epoch_loss / len(val_loader_prior))\n",
    "    hist_val.append(epoch_loss)\n",
    "\n",
    "plt.plot(hist_val, label ='validation')\n",
    "plt.plot(hist_train, label ='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.savefig('images/'+str(samples)+'_'+str(epsilon)+'_'+str(n)+\"_\"+str(hidden_size)+\"_Loss_Training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on train objects: 99 %\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in train_loader_prior:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net_prior(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on train objects: %d %%' % (100 * correct / total))\n",
    "acc_training = 100 *correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Training----------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Broadcast function not implemented for CPU tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-4a3974d9eb0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# zero the gradient buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0mreplicas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplicas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(self, module, device_ids)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mreplicate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/replicate.py\u001b[0m in \u001b[0;36mreplicate\u001b[0;34m(network, devices, detach)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mparam_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mparam_copies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBroadcast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         param_copies = [param_copies[i:i + len(params)]\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/parallel/_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, target_gpus, *inputs)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Broadcast function not implemented for CPU tensors'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mtarget_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_gpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_gpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Broadcast function not implemented for CPU tensors"
     ]
    }
   ],
   "source": [
    "# Train the Model   \n",
    "hist_train = []\n",
    "hist_val = []\n",
    "min_val_loss = 100000\n",
    "n_epochs_stop = 10\n",
    "epochs_no_improve=0\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    print('-----------Training----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    epoch_loss_val=0.0\n",
    "    running_loss_val=0.0\n",
    "    for i, (star, labels) in enumerate(train_loader):\n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs, _, _ = net(star)\n",
    "\n",
    "        if epoch > 2:\n",
    "            loss = custom_loss(outputs, labels.long(), weigths = net.fc1.weight.data,\n",
    "                               weigths_prior= net_prior.fc1.weight.data)\n",
    "        else: \n",
    "            loss = custom_loss(outputs, labels.long())\n",
    "                       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += outputs.shape[0] * loss.item()      \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "    hist_train.append(epoch_loss)\n",
    "    \n",
    "    \n",
    "    print(epoch+1, epoch_loss / len(train_loader))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('-----------Validation----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (star, labels) in enumerate(val_loader):  \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs, _, _ = net(star)\n",
    "        \n",
    "        if epoch > 2:\n",
    "            val_loss = custom_loss(outputs, labels.long(), weigths = net.fc3.weight.data,\n",
    "                               weigths_prior= net_prior.fc3.weight.data)\n",
    "        else: \n",
    "            val_loss = custom_loss(outputs, labels.long())\n",
    "            \n",
    "        epoch_loss_val += outputs.shape[0] * val_loss.item()      \n",
    "        running_loss_val += val_loss.item()\n",
    "        \n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss_val / 2000))\n",
    "            running_loss = 0.0\n",
    "   \n",
    "    print(epoch+1, epoch_loss_val / len(val_loader))\n",
    "    hist_val.append(epoch_loss_val)\n",
    "    \n",
    "    print(min_val_loss)\n",
    "    print(epoch_loss_val)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in test_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on test objects: %d %%' % (100 * correct / total))\n",
    "acc_testing = 100 *correct / total\n",
    "print(np.asarray(acc_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in train_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on train objects: %d %%' % (100 * correct / total))\n",
    "acc_training = 100 *correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_val, label ='validation')\n",
    "plt.plot(hist_train, label ='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.savefig('images/'+str(samples)+'_'+str(epsilon)+'_'+str(n)+\"_\"+str(hidden_size)+\"_Loss_Training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open(\"size_MLP_noise.csv\", \"a\")\n",
    "csv_file.write(str(np.asarray(acc_testing))+\",\"+str(np.asarray(acc_training))+\",\"+str(samples)+\",\"+str(epsilon)+\",\"+str(n)+\",\"+str(hidden_size)+\"\\n\")\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, intermediates, intermediates2, labels = get_representations(net, train_loader, device)\n",
    "outputs_test, intermediates_test, intermediates2_test, labels_test = get_representations(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pca_data, intermediate_pca_data_test = get_pca(intermediates, data_test=intermediates_test)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "\n",
    "plot_representations(intermediate_pca_data, labels, axs[0, 0])\n",
    "plot_representations(intermediate_pca_data_test, labels_test, axs[1, 0])\n",
    "\n",
    "intermediate2_pca_data, intermediate2_pca_data_test = get_pca(intermediates2, data_test=intermediates2_test)\n",
    "plot_representations(intermediate2_pca_data, labels, axs[0, 1])\n",
    "plot_representations(intermediate2_pca_data_test, labels_test, axs[1, 1])\n",
    "\n",
    "output_pca_data, output_pca_data_test = get_pca(outputs, data_test=outputs_test)\n",
    "plot_representations(output_pca_data, labels, axs[0, 2])\n",
    "plot_representations(output_pca_data_test, labels_test, axs[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CURVES = 25000\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "intermediate_tsne_data, intermediate_tsne_data_test = get_tsne(intermediates, data_test= intermediates_test, n_curves = N_CURVES)\n",
    "plot_representations(intermediate_tsne_data, labels, axs[0, 0],  n_curves = N_CURVES)\n",
    "plot_representations(intermediate_tsne_data_test, labels_test, axs[1, 0], n_curves = N_CURVES)\n",
    "\n",
    "intermediate2_tsne_data, intermediate2_tsne_data_test = get_tsne(intermediates2, data_test=intermediates2_test, n_curves = N_CURVES)\n",
    "plot_representations(intermediate2_tsne_data, labels, axs[0, 1], n_curves = N_CURVES)\n",
    "plot_representations(intermediate2_tsne_data_test, labels_test, axs[1, 1], n_curves = N_CURVES)\n",
    "\n",
    "output_tsne_data, output2_tsne_data_test = get_tsne(outputs, data_test=outputs_test, n_curves = N_CURVES)\n",
    "plot_representations(output_tsne_data, labels, axs[0, 2], n_curves = N_CURVES)\n",
    "plot_representations(output2_tsne_data_test, labels_test, axs[1, 2], n_curves = N_CURVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\n",
    "curves, labels, probs_train = get_predictions(net, train_loader_pred, device)\n",
    "pred_labels = torch.argmax(probs_train, 1)\n",
    "plot_confusion_matrix(np.round(labels), pred_labels, ax1)\n",
    "curves, labels, probs_test = get_predictions(net, test_loader_pred, device)\n",
    "pred_labels = torch.argmax(probs_test, 1)\n",
    "plot_confusion_matrix(np.round(labels), pred_labels, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_tensor))\n",
    "print(len(train_tensor_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves, labels, probs_train_sample = get_predictions(net, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "ax1.hist(probs_train[:,0], color='black')\n",
    "ax1.set_xlabel('training set')\n",
    "ax2.hist(probs_train_sample[:,0], color='black')\n",
    "ax2.set_xlabel('training set + samples')\n",
    "ax3.hist(probs_test[:,0], color='black')\n",
    "ax3.set_xlabel('testing set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(softmax(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WEIGHTS = 25\n",
    "weights = net.fc2.weight.data\n",
    "plot_weights(weights, N_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = net.fc1.weight.data\n",
    "w1 = weights1.cpu().numpy().reshape(-1,1)\n",
    "weights2 = net.fc2.weight.data\n",
    "w2 = weights2.cpu().numpy().reshape(-1,1)\n",
    "weights3 = net.fc3.weight.data\n",
    "w3 = weights3.cpu().numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "\n",
    "ax1.hist(w1, color='black')\n",
    "ax1.set_xlabel('Layer 1')\n",
    "ax2.hist(w2, color='black')\n",
    "ax2.set_xlabel('Layer 2')\n",
    "ax3.hist(w3, color='black')\n",
    "ax3.set_xlabel('Layer 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

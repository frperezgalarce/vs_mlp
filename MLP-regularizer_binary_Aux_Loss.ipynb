{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.functional as f \n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "from sklearn import manifold\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, pred_labels, ax):\n",
    "    #fig = plt.figure(figsize = (10, 10));\n",
    "    #ax = fig.add_subplot(1, 1, 1);\n",
    "    cm = metrics.confusion_matrix(labels, pred_labels, normalize='pred');\n",
    "    cm = metrics.ConfusionMatrixDisplay(cm);\n",
    "    cm.plot(cmap = 'Blues', ax = ax)\n",
    "    cm.im_.colorbar.remove()\n",
    "\n",
    "\n",
    "def get_predictions(model, iterator, device):\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred, _, _ = model(x)\n",
    "\n",
    "            y_prob = f.softmax(y_pred, dim = -1)\n",
    "            top_pred = y_prob.argmax(1, keepdim = True)\n",
    "\n",
    "            images.append(x.cpu())\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "    probs = torch.cat(probs, dim = 0)\n",
    "\n",
    "    return images, labels, probs\n",
    "\n",
    "def regularization_method(params):\n",
    "    l1_regularization = 0\n",
    "    l2_regularization = 0\n",
    "    lambda1 = 0.001\n",
    "    lambda2 = 0.001\n",
    "    for param in params:\n",
    "        l1_regularization += torch.norm(param, 1)**2\n",
    "        l2_regularization += torch.norm(param, 2)**2\n",
    "    loss = loss + lambda1*l1_regularization + lambda2*l2_regularization\n",
    "    \n",
    "def plot_weights(weights, n_weights):\n",
    "\n",
    "    rows = int(np.sqrt(n_weights))\n",
    "    cols = int(np.sqrt(n_weights))\n",
    "\n",
    "    fig = plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    for i in range(rows*cols):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(weights[i].view(5, 10).cpu().numpy(), cmap = 'bone')\n",
    "        #plt.title(str(train_target[i]))\n",
    "        ax.axis('off')\n",
    "        \n",
    "def get_pca(data, data_test=None, n_components = 2):\n",
    "    pca = decomposition.PCA()\n",
    "    \n",
    "    pca.n_components = n_components\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    \n",
    "    if data_test is not None: \n",
    "        pca_data_test = pca.transform(data_test)\n",
    "        return pca_data, pca_data_test \n",
    "    \n",
    "    return pca_data\n",
    "\n",
    "\n",
    "def get_tsne(data, data_test = None, n_components = 2, n_curves = None):\n",
    "    if n_curves is not None:\n",
    "        data = data[:n_curves]\n",
    "    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    \n",
    "    if data_test is not None: \n",
    "        tsne_data_test = tsne.fit_transform(data_test)\n",
    "        return tsne_data, tsne_data_test  \n",
    "    \n",
    "    return tsne_data\n",
    "\n",
    "def get_representations(model, iterator, device):\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    intermediates = []\n",
    "    intermediates2 = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in iterator:\n",
    "            x = x.to(device)\n",
    "            y_pred, h2, h1 = net(x)\n",
    "            outputs.append(y_pred.cpu())\n",
    "            intermediates.append(h1.cpu())\n",
    "            intermediates2.append(h2.cpu())\n",
    "            labels.append(y)\n",
    "        \n",
    "    outputs = torch.cat(outputs, dim = 0)\n",
    "    intermediates = torch.cat(intermediates, dim = 0)\n",
    "    intermediates2 = torch.cat(intermediates2, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "\n",
    "    return outputs, intermediates, intermediates2, labels\n",
    "\n",
    "def plot_representations(data, labels, ax, n_curves = None):\n",
    "    if n_curves is not None:\n",
    "        data = data[:n_curves]\n",
    "        labels = labels[:n_curves]\n",
    "    #fig = plt.figure(figsize = (10, 10))\n",
    "    #ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, alpha =0.5)\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    legend = ax.legend(handles = handles, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 1\n",
    "fileTrain = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Train_rrlyr-'+str(number)+'.csv'\n",
    "fileTest = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Test_rrlyr-'+str(number)+'.csv'\n",
    "train_dataset = pd.read_csv(fileTrain, index_col ='Unnamed: 0')\n",
    "test_dataset = pd.read_csv(fileTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq1_harmonics_rel_phase_0\n",
      "Freq2_harmonics_rel_phase_0\n",
      "Freq3_harmonics_rel_phase_0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_dataset =  train_dataset.drop(['Pred', 'Pred2', 'h', 'e', 'u','ID'], axis = 1)\n",
    "    for col in train_dataset.columns:\n",
    "        if col not in ['label']:\n",
    "            if train_dataset[col].var()==0:\n",
    "                print(col)\n",
    "                del train_dataset[col]\n",
    "    test_dataset = test_dataset[list(train_dataset.columns)]\n",
    "except:\n",
    "    print(col)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_dataset.shape[0] \n",
    "epsilon = 0\n",
    "input_size = train_dataset.shape[1]-1\n",
    "hidden_size = 50\n",
    "hidden_size2 = 50\n",
    "num_classes = 2\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "regularization = False\n",
    "add_DR_based_data = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(391501, 61)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.sample(n)\n",
    "train_dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28625, 61)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset['PeriodLS'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(306844, 61)\n"
     ]
    }
   ],
   "source": [
    "label = train_dataset['label']\n",
    "del train_dataset['label']\n",
    "\n",
    "\n",
    "train_dataset_z=(train_dataset-train_dataset.mean())/train_dataset.std()\n",
    "z_scores = stats.zscore(train_dataset_z)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "train_dataset['label'] = label\n",
    "train_dataset = train_dataset[filtered_entries]\n",
    "print(train_dataset.shape)\n",
    "train_dataset_pred = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28625, 60)\n",
      "(28625, 61)\n"
     ]
    }
   ],
   "source": [
    "label = test_dataset['label']\n",
    "del test_dataset['label']\n",
    "\n",
    "print(test_dataset.shape)\n",
    "\n",
    "test_dataset_z=(test_dataset-test_dataset.mean())/test_dataset.std()\n",
    "z_scores = stats.zscore(test_dataset_z)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "test_dataset['label'] = label\n",
    "\n",
    "print(test_dataset.shape)\n",
    "\n",
    "test_dataset = test_dataset[filtered_entries]\n",
    "\n",
    "test_dataset_pred = test_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Amplitude', 'AndersonDarling', 'Autocor_length', 'Beyond1Std',\n",
       "       'CAR_mean', 'CAR_sigma', 'CAR_tau', 'Con', 'Eta_e',\n",
       "       'FluxPercentileRatioMid20', 'FluxPercentileRatioMid35',\n",
       "       'FluxPercentileRatioMid50', 'FluxPercentileRatioMid65',\n",
       "       'FluxPercentileRatioMid80', 'Freq1_harmonics_amplitude_0',\n",
       "       'Freq1_harmonics_amplitude_1', 'Freq1_harmonics_amplitude_2',\n",
       "       'Freq1_harmonics_amplitude_3', 'Freq1_harmonics_rel_phase_1',\n",
       "       'Freq1_harmonics_rel_phase_2', 'Freq1_harmonics_rel_phase_3',\n",
       "       'Freq2_harmonics_amplitude_0', 'Freq2_harmonics_amplitude_1',\n",
       "       'Freq2_harmonics_amplitude_2', 'Freq2_harmonics_amplitude_3',\n",
       "       'Freq2_harmonics_rel_phase_1', 'Freq2_harmonics_rel_phase_2',\n",
       "       'Freq2_harmonics_rel_phase_3', 'Freq3_harmonics_amplitude_0',\n",
       "       'Freq3_harmonics_amplitude_1', 'Freq3_harmonics_amplitude_2',\n",
       "       'Freq3_harmonics_amplitude_3', 'Freq3_harmonics_rel_phase_1',\n",
       "       'Freq3_harmonics_rel_phase_2', 'Freq3_harmonics_rel_phase_3', 'Gskew',\n",
       "       'LinearTrend', 'MaxSlope', 'Mean', 'Meanvariance', 'MedianAbsDev',\n",
       "       'MedianBRP', 'PairSlopeTrend', 'PercentAmplitude',\n",
       "       'PercentDifferenceFluxPercentile', 'PeriodLS', 'Period_fit', 'Psi_CS',\n",
       "       'Psi_eta', 'Q31', 'Rcs', 'Skew', 'SlottedA_length', 'SmallKurtosis',\n",
       "       'Std', 'StetsonK', 'StetsonK_AC', 'StructureFunction_index_21',\n",
       "       'StructureFunction_index_31', 'StructureFunction_index_32', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>FluxPercentileRatioMid20</th>\n",
       "      <th>...</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>391262</th>\n",
       "      <td>0.15100</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>10</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>22.791959</td>\n",
       "      <td>0.537868</td>\n",
       "      <td>0.619429</td>\n",
       "      <td>0.02349</td>\n",
       "      <td>14.104876</td>\n",
       "      <td>0.145522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194838</td>\n",
       "      <td>17.79498</td>\n",
       "      <td>-0.488208</td>\n",
       "      <td>0.078843</td>\n",
       "      <td>0.814231</td>\n",
       "      <td>0.784707</td>\n",
       "      <td>1.801910</td>\n",
       "      <td>2.535099</td>\n",
       "      <td>1.448028</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230291</th>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.115532</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>274.744191</td>\n",
       "      <td>0.211135</td>\n",
       "      <td>0.053812</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.686240</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133719</td>\n",
       "      <td>5.69361</td>\n",
       "      <td>0.111186</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.793480</td>\n",
       "      <td>0.745731</td>\n",
       "      <td>1.620815</td>\n",
       "      <td>2.129265</td>\n",
       "      <td>1.607298</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26148</th>\n",
       "      <td>0.02575</td>\n",
       "      <td>0.536316</td>\n",
       "      <td>2</td>\n",
       "      <td>0.299065</td>\n",
       "      <td>39.534998</td>\n",
       "      <td>0.553698</td>\n",
       "      <td>0.389701</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6307.485427</td>\n",
       "      <td>0.186047</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058798</td>\n",
       "      <td>4.92600</td>\n",
       "      <td>-0.091798</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.791180</td>\n",
       "      <td>0.738202</td>\n",
       "      <td>1.453116</td>\n",
       "      <td>1.593353</td>\n",
       "      <td>1.206530</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34003</th>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.253883</td>\n",
       "      <td>5</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>206.752429</td>\n",
       "      <td>0.049652</td>\n",
       "      <td>0.068576</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.364235</td>\n",
       "      <td>0.156250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502323</td>\n",
       "      <td>1.18029</td>\n",
       "      <td>1.457469</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>0.777735</td>\n",
       "      <td>0.741196</td>\n",
       "      <td>1.317766</td>\n",
       "      <td>1.621916</td>\n",
       "      <td>1.385394</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189282</th>\n",
       "      <td>0.45500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>18.179650</td>\n",
       "      <td>0.189635</td>\n",
       "      <td>1.087083</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.117858</td>\n",
       "      <td>0.088483</td>\n",
       "      <td>...</td>\n",
       "      <td>1.129390</td>\n",
       "      <td>1.89933</td>\n",
       "      <td>2.278362</td>\n",
       "      <td>0.219409</td>\n",
       "      <td>0.742077</td>\n",
       "      <td>0.726959</td>\n",
       "      <td>2.211178</td>\n",
       "      <td>3.532070</td>\n",
       "      <td>1.632040</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Amplitude  AndersonDarling  Autocor_length  Beyond1Std    CAR_mean  \\\n",
       "391262    0.15100         0.001250              10    0.346667   22.791959   \n",
       "230291    0.01750         0.115532               2    0.363333  274.744191   \n",
       "26148     0.02575         0.536316               2    0.299065   39.534998   \n",
       "34003     0.01825         0.253883               5    0.283186  206.752429   \n",
       "189282    0.45500         0.000000               1    0.220000   18.179650   \n",
       "\n",
       "        CAR_sigma   CAR_tau      Con        Eta_e  FluxPercentileRatioMid20  \\\n",
       "391262   0.537868  0.619429  0.02349    14.104876                  0.145522   \n",
       "230291   0.211135  0.053812  0.00000     5.686240                  0.166667   \n",
       "26148    0.553698  0.389701  0.00000  6307.485427                  0.186047   \n",
       "34003    0.049652  0.068576  0.00000     8.364235                  0.156250   \n",
       "189282   0.189635  1.087083  0.00000    17.117858                  0.088483   \n",
       "\n",
       "        ...      Skew  SlottedA_length  SmallKurtosis       Std  StetsonK  \\\n",
       "391262  ...  0.194838         17.79498      -0.488208  0.078843  0.814231   \n",
       "230291  ...  0.133719          5.69361       0.111186  0.009347  0.793480   \n",
       "26148   ... -0.058798          4.92600      -0.091798  0.012686  0.791180   \n",
       "34003   ... -0.502323          1.18029       1.457469  0.010113  0.777735   \n",
       "189282  ...  1.129390          1.89933       2.278362  0.219409  0.742077   \n",
       "\n",
       "        StetsonK_AC  StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "391262     0.784707                    1.801910                    2.535099   \n",
       "230291     0.745731                    1.620815                    2.129265   \n",
       "26148      0.738202                    1.453116                    1.593353   \n",
       "34003      0.741196                    1.317766                    1.621916   \n",
       "189282     0.726959                    2.211178                    3.532070   \n",
       "\n",
       "        StructureFunction_index_32   label  \n",
       "391262                    1.448028  ClassB  \n",
       "230291                    1.607298  ClassB  \n",
       "26148                     1.206530  ClassB  \n",
       "34003                     1.385394  ClassB  \n",
       "189282                    1.632040  ClassB  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 3000\n",
    "samples1 = samples*2\n",
    "number_columns = train_dataset.shape[1]\n",
    "option = 2\n",
    "\n",
    "\n",
    "data_prior = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns)\n",
    "\n",
    "if add_DR_based_data:\n",
    "    #option 1\n",
    "    if option == 1:\n",
    "        for i in range(samples1):\n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']= (np.random.uniform(0.2-epsilon,1.0+epsilon))#-minimum_period)/(maximum_period-minimum_period)\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    if option==2:\n",
    "        #option 2\n",
    "        for i in range(samples):\n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']=(np.random.uniform(0.1,0.2))#-minimum_period)/(maximum_period-minimum_period)\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']=(np.random.uniform(1.0,1.1))\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        \n",
    "    #option 3\n",
    "    if option==3:\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) #pd.DataFrame([train_dataset.sample(1000).mean()]).T\n",
    "            new_data['PeriodLS']= 1.0\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']= 0.2\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior, val_dataset_prior = train_test_split(data_prior, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "\n",
    "train_target_prior = torch.tensor(train_dataset_prior['label'].values.astype(np.float32))\n",
    "train_prior = torch.tensor(train_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_prior = f.normalize(train_prior)\n",
    "train_tensor_prior = data_utils.TensorDataset(train_prior, train_target_prior) \n",
    "train_loader_prior = data_utils.DataLoader(dataset = train_tensor_prior, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "val_target_prior = torch.tensor(val_dataset_prior['label'].values.astype(np.float32))\n",
    "val_prior = torch.tensor(val_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val_prior = f.normalize(val_prior)\n",
    "val_tensor_prior = data_utils.TensorDataset(val_prior, val_target_prior) \n",
    "val_loader_prior = data_utils.DataLoader(dataset = val_tensor_prior, batch_size = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases using DR 1: Period $ \\in [0.2,1.0]$ days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassA', '1')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassB', '0')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('Noise', '0.5')\n",
    "train_target = torch.tensor(train_dataset['label'].values.astype(np.float32))\n",
    "train = torch.tensor(train_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train = f.normalize(train)\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "train_target_pred = torch.tensor(train_dataset_pred['label'].values.astype(np.float32))\n",
    "train_pred = torch.tensor(train_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_pred = f.normalize(train_pred)\n",
    "train_tensor_pred = data_utils.TensorDataset(train_pred, train_target_pred) \n",
    "train_loader_pred = data_utils.DataLoader(dataset = train_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassA', '1')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassB', '0')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('Noise', '0.5')\n",
    "val_target = torch.tensor(val_dataset['label'].values.astype(np.float32))\n",
    "val = torch.tensor(val_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val = f.normalize(val)\n",
    "val_tensor = data_utils.TensorDataset(val, val_target) \n",
    "val_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassA', '1')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassB', '0')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('Noise', '0.5')\n",
    "test_target = torch.tensor(test_dataset['label'].values.astype(np.float32))\n",
    "test = torch.tensor(test_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test = f.normalize(test)\n",
    "test_tensor = data_utils.TensorDataset(test, test_target) \n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "test_target_pred = torch.tensor(test_dataset_pred['label'].values.astype(np.float32))\n",
    "test_pred = torch.tensor(test_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test_pred = f.normalize(test_pred)\n",
    "test_tensor_pred = data_utils.TensorDataset(test_pred, test_target_pred) \n",
    "test_loader_pred = data_utils.DataLoader(dataset = test_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        #self.relu = nn.ReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size2)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes) \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.fc1(x)\n",
    "        #out = self.batchnorm1(out)\n",
    "        #out = self.relu(out)\n",
    "        out = self.relu(out1)\n",
    "        out2 = self.fc2(out)\n",
    "        #out = self.batchnorm1(out)\n",
    "        out = self.relu2(out)\n",
    "        out3 = self.fc3(out)\n",
    "        #x = self.dropout(x)\n",
    "        #out = self.sigmoid(out)\n",
    "        return out3, out2, out1\n",
    "  \n",
    "#net_prior = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "#net_prior.cuda()\n",
    "\n",
    "net = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "\n",
    "net.cuda()\n",
    "#net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, labels, weigths=None, weigths_prior=None):\n",
    "    regularization_loss = 0\n",
    "    lambda_1 = 0.00001\n",
    "    if weigths is not None: \n",
    "        regularization_loss += fast_cdist(weigths, weigths_prior)\n",
    "            #print(regularization_loss)\n",
    "        loss = criterion(outputs, labels) + lambda_1*regularization_loss #nn.L1Loss()(weigths, weigths)\n",
    "    else:\n",
    "        loss = criterion(outputs, labels)\n",
    "        #print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_cdist(x1, x2):\n",
    "    res=f.mse_loss(x1, x2, size_average=False)\n",
    "    #res=f.l1_loss(x1, x2, size_average=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "# Train the Model   \n",
    "hist_train = []\n",
    "hist_val = []\n",
    "num_epochs_prior = 50\n",
    "for epoch in range(num_epochs_prior):\n",
    "    print('-----------Training----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    epoch_loss_prior = 0.0    \n",
    "    running_loss_prior = 0.0\n",
    "    \n",
    "    \n",
    "    for item1, item2 in zip(train_loader_prior, cycle(train_loader)):\n",
    "        star_prior, labels_prior = item1\n",
    "        star, labels = item2\n",
    "        \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  \n",
    "        outputs, _, _ = net(star)\n",
    "        loss = criterion(outputs, labels.long())      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += outputs.shape[0] * loss.item()      \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "        running_loss = 0.0\n",
    "    \n",
    "    \n",
    "\n",
    "    hist_train.append(epoch_loss)    \n",
    "    print(epoch+1, epoch_loss / len(train_loader))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('-----------Validation----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (star, labels) in enumerate(val_loader):  \n",
    "        \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  \n",
    "        outputs, _, _ = net(star)\n",
    "        loss = criterion(outputs, labels.long())      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += outputs.shape[0] * loss.item()      \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if i % 200 == 199:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "        running_loss = 0.0\n",
    "\n",
    "   \n",
    "    print(epoch+1, epoch_loss / len(val_loader))\n",
    "    hist_val.append(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in test_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on test objects: %d %%' % (100 * correct / total))\n",
    "acc_testing = 100 *correct / total\n",
    "print(np.asarray(acc_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in train_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on train objects: %d %%' % (100 * correct / total))\n",
    "acc_training = 100 *correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_val, label ='validation')\n",
    "plt.plot(hist_train, label ='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.savefig('images/'+str(samples)+'_'+str(epsilon)+'_'+str(n)+\"_\"+str(hidden_size)+\"_Loss_Training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open(\"size_MLP_noise.csv\", \"a\")\n",
    "csv_file.write(str(np.asarray(acc_testing))+\",\"+str(np.asarray(acc_training))+\",\"+str(samples)+\",\"+str(epsilon)+\",\"+str(n)+\",\"+str(hidden_size)+\"\\n\")\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, intermediates, intermediates2, labels = get_representations(net, train_loader, device)\n",
    "outputs_test, intermediates_test, intermediates2_test, labels_test = get_representations(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pca_data, intermediate_pca_data_test = get_pca(intermediates, data_test=intermediates_test)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "\n",
    "plot_representations(intermediate_pca_data, labels, axs[0, 0])\n",
    "plot_representations(intermediate_pca_data_test, labels_test, axs[1, 0])\n",
    "\n",
    "intermediate2_pca_data, intermediate2_pca_data_test = get_pca(intermediates2, data_test=intermediates2_test)\n",
    "plot_representations(intermediate2_pca_data, labels, axs[0, 1])\n",
    "plot_representations(intermediate2_pca_data_test, labels_test, axs[1, 1])\n",
    "\n",
    "output_pca_data, output_pca_data_test = get_pca(outputs, data_test=outputs_test)\n",
    "plot_representations(output_pca_data, labels, axs[0, 2])\n",
    "plot_representations(output_pca_data_test, labels_test, axs[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CURVES = 25000\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "intermediate_tsne_data, intermediate_tsne_data_test = get_tsne(intermediates, data_test= intermediates_test, n_curves = N_CURVES)\n",
    "plot_representations(intermediate_tsne_data, labels, axs[0, 0],  n_curves = N_CURVES)\n",
    "plot_representations(intermediate_tsne_data_test, labels_test, axs[1, 0], n_curves = N_CURVES)\n",
    "\n",
    "intermediate2_tsne_data, intermediate2_tsne_data_test = get_tsne(intermediates2, data_test=intermediates2_test, n_curves = N_CURVES)\n",
    "plot_representations(intermediate2_tsne_data, labels, axs[0, 1], n_curves = N_CURVES)\n",
    "plot_representations(intermediate2_tsne_data_test, labels_test, axs[1, 1], n_curves = N_CURVES)\n",
    "\n",
    "output_tsne_data, output2_tsne_data_test = get_tsne(outputs, data_test=outputs_test, n_curves = N_CURVES)\n",
    "plot_representations(output_tsne_data, labels, axs[0, 2], n_curves = N_CURVES)\n",
    "plot_representations(output2_tsne_data_test, labels_test, axs[1, 2], n_curves = N_CURVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\n",
    "curves, labels, probs_train = get_predictions(net, train_loader_pred, device)\n",
    "pred_labels = torch.argmax(probs_train, 1)\n",
    "plot_confusion_matrix(np.round(labels), pred_labels, ax1)\n",
    "curves, labels, probs_test = get_predictions(net, test_loader_pred, device)\n",
    "pred_labels = torch.argmax(probs_test, 1)\n",
    "plot_confusion_matrix(np.round(labels), pred_labels, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_tensor))\n",
    "print(len(train_tensor_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves, labels, probs_train_sample = get_predictions(net, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "ax1.hist(probs_train[:,0], color='black')\n",
    "ax1.set_xlabel('training set')\n",
    "ax2.hist(probs_train_sample[:,0], color='black')\n",
    "ax2.set_xlabel('training set + samples')\n",
    "ax3.hist(probs_test[:,0], color='black')\n",
    "ax3.set_xlabel('testing set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(softmax(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WEIGHTS = 25\n",
    "weights = net.fc2.weight.data\n",
    "plot_weights(weights, N_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = net.fc1.weight.data\n",
    "w1 = weights1.cpu().numpy().reshape(-1,1)\n",
    "weights2 = net.fc2.weight.data\n",
    "w2 = weights2.cpu().numpy().reshape(-1,1)\n",
    "weights3 = net.fc3.weight.data\n",
    "w3 = weights3.cpu().numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "\n",
    "ax1.hist(w1, color='black')\n",
    "ax1.set_xlabel('Layer 1')\n",
    "ax2.hist(w2, color='black')\n",
    "ax2.set_xlabel('Layer 2')\n",
    "ax3.hist(w3, color='black')\n",
    "ax3.set_xlabel('Layer 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.functional as f \n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 1\n",
    "fileTrain = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Train_rrlyr-'+str(number)+'.csv'\n",
    "fileTest = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Test_rrlyr-'+str(number)+'.csv'\n",
    "train_dataset = pd.read_csv(fileTrain, index_col ='Unnamed: 0')\n",
    "test_dataset = pd.read_csv(fileTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_dataset =  train_dataset.drop(['Pred', 'Pred2', 'h', 'e', 'u','ID'], axis = 1)\n",
    "    test_dataset = test_dataset[list(train_dataset.columns)]\n",
    "except:\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = train_dataset.shape[0] \n",
    "input_size = train_dataset.shape[1]-1\n",
    "hidden_size = 100\n",
    "hidden_size2 = 100\n",
    "num_classes = 2\n",
    "num_epochs = 50\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "regularization = False\n",
    "add_DR_based_data = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>FluxPercentileRatioMid20</th>\n",
       "      <th>...</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.514987</td>\n",
       "      <td>0.107490</td>\n",
       "      <td>-0.284489</td>\n",
       "      <td>0.333715</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>0.816393</td>\n",
       "      <td>-0.033293</td>\n",
       "      <td>-0.238377</td>\n",
       "      <td>-0.070306</td>\n",
       "      <td>0.289010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054652</td>\n",
       "      <td>-0.096971</td>\n",
       "      <td>-0.261869</td>\n",
       "      <td>-0.488317</td>\n",
       "      <td>0.309904</td>\n",
       "      <td>0.248875</td>\n",
       "      <td>-0.842580</td>\n",
       "      <td>-0.714159</td>\n",
       "      <td>0.342209</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.412166</td>\n",
       "      <td>-0.353378</td>\n",
       "      <td>0.894954</td>\n",
       "      <td>0.333715</td>\n",
       "      <td>-0.001798</td>\n",
       "      <td>1.124202</td>\n",
       "      <td>-0.033620</td>\n",
       "      <td>-0.531536</td>\n",
       "      <td>-0.062992</td>\n",
       "      <td>0.252125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051096</td>\n",
       "      <td>-0.491377</td>\n",
       "      <td>-0.358583</td>\n",
       "      <td>-0.396467</td>\n",
       "      <td>0.450689</td>\n",
       "      <td>-0.398543</td>\n",
       "      <td>0.843398</td>\n",
       "      <td>0.885671</td>\n",
       "      <td>0.783394</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.572823</td>\n",
       "      <td>1.422167</td>\n",
       "      <td>-0.621473</td>\n",
       "      <td>-0.099947</td>\n",
       "      <td>-0.001881</td>\n",
       "      <td>-0.680380</td>\n",
       "      <td>-0.033083</td>\n",
       "      <td>-0.531536</td>\n",
       "      <td>-0.070461</td>\n",
       "      <td>0.201736</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.135977</td>\n",
       "      <td>-0.294606</td>\n",
       "      <td>-0.054042</td>\n",
       "      <td>-0.546505</td>\n",
       "      <td>-0.290366</td>\n",
       "      <td>-0.201901</td>\n",
       "      <td>0.178371</td>\n",
       "      <td>0.185648</td>\n",
       "      <td>0.252165</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.470002</td>\n",
       "      <td>0.252973</td>\n",
       "      <td>-0.284489</td>\n",
       "      <td>0.220974</td>\n",
       "      <td>-0.001863</td>\n",
       "      <td>-0.510309</td>\n",
       "      <td>-0.033337</td>\n",
       "      <td>-0.531536</td>\n",
       "      <td>-0.062328</td>\n",
       "      <td>-0.195858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143167</td>\n",
       "      <td>-0.408632</td>\n",
       "      <td>-0.236068</td>\n",
       "      <td>-0.448973</td>\n",
       "      <td>0.108854</td>\n",
       "      <td>-0.443699</td>\n",
       "      <td>-0.356354</td>\n",
       "      <td>-0.466542</td>\n",
       "      <td>-0.910932</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.482855</td>\n",
       "      <td>-0.338269</td>\n",
       "      <td>-0.284489</td>\n",
       "      <td>0.164587</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>1.536734</td>\n",
       "      <td>-0.033180</td>\n",
       "      <td>-0.238377</td>\n",
       "      <td>-0.071884</td>\n",
       "      <td>0.277867</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.774369</td>\n",
       "      <td>-0.054457</td>\n",
       "      <td>-0.275064</td>\n",
       "      <td>-0.468088</td>\n",
       "      <td>0.182466</td>\n",
       "      <td>1.073983</td>\n",
       "      <td>1.116882</td>\n",
       "      <td>1.095356</td>\n",
       "      <td>0.617891</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amplitude  AndersonDarling  Autocor_length  Beyond1Std  CAR_mean  \\\n",
       "0  -0.514987         0.107490       -0.284489    0.333715 -0.001869   \n",
       "1  -0.412166        -0.353378        0.894954    0.333715 -0.001798   \n",
       "2  -0.572823         1.422167       -0.621473   -0.099947 -0.001881   \n",
       "3  -0.470002         0.252973       -0.284489    0.220974 -0.001863   \n",
       "4  -0.482855        -0.338269       -0.284489    0.164587 -0.001877   \n",
       "\n",
       "   CAR_sigma   CAR_tau       Con     Eta_e  FluxPercentileRatioMid20  ...  \\\n",
       "0   0.816393 -0.033293 -0.238377 -0.070306                  0.289010  ...   \n",
       "1   1.124202 -0.033620 -0.531536 -0.062992                  0.252125  ...   \n",
       "2  -0.680380 -0.033083 -0.531536 -0.070461                  0.201736  ...   \n",
       "3  -0.510309 -0.033337 -0.531536 -0.062328                 -0.195858  ...   \n",
       "4   1.536734 -0.033180 -0.238377 -0.071884                  0.277867  ...   \n",
       "\n",
       "       Skew  SlottedA_length  SmallKurtosis       Std  StetsonK  StetsonK_AC  \\\n",
       "0  0.054652        -0.096971      -0.261869 -0.488317  0.309904     0.248875   \n",
       "1  0.051096        -0.491377      -0.358583 -0.396467  0.450689    -0.398543   \n",
       "2 -0.135977        -0.294606      -0.054042 -0.546505 -0.290366    -0.201901   \n",
       "3  0.143167        -0.408632      -0.236068 -0.448973  0.108854    -0.443699   \n",
       "4 -0.774369        -0.054457      -0.275064 -0.468088  0.182466     1.073983   \n",
       "\n",
       "   StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "0                   -0.842580                   -0.714159   \n",
       "1                    0.843398                    0.885671   \n",
       "2                    0.178371                    0.185648   \n",
       "3                   -0.356354                   -0.466542   \n",
       "4                    1.116882                    1.095356   \n",
       "\n",
       "   StructureFunction_index_32   label  \n",
       "0                    0.342209  ClassB  \n",
       "1                    0.783394  ClassB  \n",
       "2                    0.252165  ClassB  \n",
       "3                   -0.910932     NaN  \n",
       "4                    0.617891  ClassB  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimum_period = train_dataset['PeriodLS'].min()\n",
    "maximum_period = train_dataset['PeriodLS'].max()\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.sample(n)\n",
    "label = train_dataset['label']\n",
    "del train_dataset['label']\n",
    "scaler = StandardScaler()\n",
    "train_dataset_scaled = scaler.fit_transform(train_dataset)\n",
    "train_dataset = pd.DataFrame(train_dataset_scaled, columns=train_dataset.columns)\n",
    "train_dataset['label'] = label\n",
    "train_dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>FluxPercentileRatioMid20</th>\n",
       "      <th>...</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.368630</td>\n",
       "      <td>-0.653029</td>\n",
       "      <td>-0.621473</td>\n",
       "      <td>-1.244811</td>\n",
       "      <td>-0.001754</td>\n",
       "      <td>-0.438519</td>\n",
       "      <td>-0.033638</td>\n",
       "      <td>-0.531536</td>\n",
       "      <td>-0.071938</td>\n",
       "      <td>-0.603972</td>\n",
       "      <td>...</td>\n",
       "      <td>1.321295</td>\n",
       "      <td>-0.345227</td>\n",
       "      <td>-0.169991</td>\n",
       "      <td>0.432532</td>\n",
       "      <td>-0.086576</td>\n",
       "      <td>1.829801</td>\n",
       "      <td>-0.118080</td>\n",
       "      <td>-0.112777</td>\n",
       "      <td>0.446760</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.056245</td>\n",
       "      <td>-0.643994</td>\n",
       "      <td>-0.621473</td>\n",
       "      <td>1.292115</td>\n",
       "      <td>-0.001854</td>\n",
       "      <td>-0.474296</td>\n",
       "      <td>-0.033354</td>\n",
       "      <td>-0.531536</td>\n",
       "      <td>-0.070766</td>\n",
       "      <td>1.925675</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.602614</td>\n",
       "      <td>-0.432620</td>\n",
       "      <td>-0.728697</td>\n",
       "      <td>1.093200</td>\n",
       "      <td>1.581364</td>\n",
       "      <td>1.247411</td>\n",
       "      <td>-0.402680</td>\n",
       "      <td>-0.319404</td>\n",
       "      <td>0.663064</td>\n",
       "      <td>ClassA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.456992</td>\n",
       "      <td>-0.652955</td>\n",
       "      <td>-0.621473</td>\n",
       "      <td>1.417879</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>0.981196</td>\n",
       "      <td>-0.033373</td>\n",
       "      <td>-0.531536</td>\n",
       "      <td>-0.041288</td>\n",
       "      <td>-0.146607</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.147137</td>\n",
       "      <td>-0.420964</td>\n",
       "      <td>-0.709481</td>\n",
       "      <td>0.640285</td>\n",
       "      <td>1.554462</td>\n",
       "      <td>1.321814</td>\n",
       "      <td>-1.269708</td>\n",
       "      <td>-1.238012</td>\n",
       "      <td>-1.546118</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.045866</td>\n",
       "      <td>-0.653029</td>\n",
       "      <td>0.557970</td>\n",
       "      <td>0.784730</td>\n",
       "      <td>-0.001780</td>\n",
       "      <td>1.432169</td>\n",
       "      <td>-0.033627</td>\n",
       "      <td>-0.238377</td>\n",
       "      <td>-0.072123</td>\n",
       "      <td>5.525472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205319</td>\n",
       "      <td>-0.489436</td>\n",
       "      <td>-0.702007</td>\n",
       "      <td>0.054557</td>\n",
       "      <td>2.397809</td>\n",
       "      <td>0.967167</td>\n",
       "      <td>1.162710</td>\n",
       "      <td>1.158074</td>\n",
       "      <td>0.714759</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.094064</td>\n",
       "      <td>-0.652326</td>\n",
       "      <td>-0.621473</td>\n",
       "      <td>-0.737426</td>\n",
       "      <td>-0.001851</td>\n",
       "      <td>0.762495</td>\n",
       "      <td>-0.033338</td>\n",
       "      <td>-0.531536</td>\n",
       "      <td>-0.071305</td>\n",
       "      <td>-0.309453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501169</td>\n",
       "      <td>-0.342244</td>\n",
       "      <td>0.012441</td>\n",
       "      <td>-0.150451</td>\n",
       "      <td>-0.070468</td>\n",
       "      <td>0.588588</td>\n",
       "      <td>0.018325</td>\n",
       "      <td>-0.107456</td>\n",
       "      <td>-0.452754</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Amplitude  AndersonDarling  Autocor_length  Beyond1Std  CAR_mean  \\\n",
       "0   0.368630        -0.653029       -0.621473   -1.244811 -0.001754   \n",
       "1   1.056245        -0.643994       -0.621473    1.292115 -0.001854   \n",
       "2   0.456992        -0.652955       -0.621473    1.417879 -0.001850   \n",
       "3  -0.045866        -0.653029        0.557970    0.784730 -0.001780   \n",
       "4  -0.094064        -0.652326       -0.621473   -0.737426 -0.001851   \n",
       "\n",
       "   CAR_sigma   CAR_tau       Con     Eta_e  FluxPercentileRatioMid20  ...  \\\n",
       "0  -0.438519 -0.033638 -0.531536 -0.071938                 -0.603972  ...   \n",
       "1  -0.474296 -0.033354 -0.531536 -0.070766                  1.925675  ...   \n",
       "2   0.981196 -0.033373 -0.531536 -0.041288                 -0.146607  ...   \n",
       "3   1.432169 -0.033627 -0.238377 -0.072123                  5.525472  ...   \n",
       "4   0.762495 -0.033338 -0.531536 -0.071305                 -0.309453  ...   \n",
       "\n",
       "       Skew  SlottedA_length  SmallKurtosis       Std  StetsonK  StetsonK_AC  \\\n",
       "0  1.321295        -0.345227      -0.169991  0.432532 -0.086576     1.829801   \n",
       "1 -0.602614        -0.432620      -0.728697  1.093200  1.581364     1.247411   \n",
       "2 -1.147137        -0.420964      -0.709481  0.640285  1.554462     1.321814   \n",
       "3  0.205319        -0.489436      -0.702007  0.054557  2.397809     0.967167   \n",
       "4  0.501169        -0.342244       0.012441 -0.150451 -0.070468     0.588588   \n",
       "\n",
       "   StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "0                   -0.118080                   -0.112777   \n",
       "1                   -0.402680                   -0.319404   \n",
       "2                   -1.269708                   -1.238012   \n",
       "3                    1.162710                    1.158074   \n",
       "4                    0.018325                   -0.107456   \n",
       "\n",
       "   StructureFunction_index_32   label  \n",
       "0                    0.446760  ClassB  \n",
       "1                    0.663064  ClassA  \n",
       "2                   -1.546118  ClassB  \n",
       "3                    0.714759  ClassB  \n",
       "4                   -0.452754  ClassB  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = test_dataset[['label']]\n",
    "del test_dataset['label']\n",
    "test_dataset_scaled = scaler.transform(test_dataset)\n",
    "test_dataset = pd.DataFrame(test_dataset_scaled, columns=test_dataset.columns)\n",
    "test_dataset['label'] = label\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1205931855941424e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.random.uniform(0.1,1.1)-minimum_period)/(maximum_period-minimum_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 500\n",
    "if add_DR_based_data:\n",
    "    for i in range(samples):\n",
    "        new_data = pd.DataFrame(train_dataset.sample(1000).mean()).T\n",
    "        new_data['PeriodLS']= (np.random.uniform(0.1,1.1)-minimum_period)/(maximum_period-minimum_period)\n",
    "        new_data['label'] = 'ClassA'\n",
    "        frames = [train_dataset, new_data]\n",
    "        train_dataset = pd.concat(frames)\n",
    "    \n",
    "    for i in range(0):\n",
    "        new_data = pd.DataFrame(train_dataset.sample(1000).mean()).T\n",
    "        new_data['PeriodLS']=(np.random.uniform(0.0,0.2)-minimum_period)/(maximum_period-minimum_period)\n",
    "        new_data['label'] = 'ClassB'\n",
    "        frames = [train_dataset, new_data]\n",
    "        train_dataset = pd.concat(frames)\n",
    "        \n",
    "    for i in range(0):    \n",
    "        new_data = pd.DataFrame(train_dataset.sample(1000).mean()).T\n",
    "        new_data['PeriodLS']=(np.random.uniform(1.0,1.2)-minimum_period)/(maximum_period-minimum_period)\n",
    "        new_data['label'] = 'ClassB'\n",
    "        frames = [train_dataset, new_data]\n",
    "        train_dataset = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases using DR 1: Period $ \\in [0.2,1.0]$ days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['label'] = 1*(train_dataset['label'] == 'ClassA')\n",
    "train_target = torch.tensor(train_dataset['label'].values.astype(np.float32))\n",
    "train = torch.tensor(train_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset['label'] = 1*(val_dataset['label'] == 'ClassA')\n",
    "val_target = torch.tensor(val_dataset['label'].values.astype(np.float32))\n",
    "val = torch.tensor(val_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val_tensor = data_utils.TensorDataset(val, val_target) \n",
    "val_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['label'] = 1*(test_dataset['label'] == 'ClassA')\n",
    "test_target = torch.tensor(test_dataset['label'].values.astype(np.float32))\n",
    "test = torch.tensor(test_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test_tensor = data_utils.TensorDataset(test, test_target) \n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc1): Linear(in_features=63, out_features=100, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=100, out_features=100, bias=True)\n",
       "  (relu2): ReLU()\n",
       "  (fc3): Linear(in_features=100, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Neural Network Model (1 hidden layer)\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size2)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "net = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "net.cuda()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------Training----------------------------\n",
      "[1,  2000] loss: 0.274\n",
      "[1,  4000] loss: 0.268\n",
      "[1,  6000] loss: 0.266\n",
      "[1,  8000] loss: 0.267\n",
      "[1, 10000] loss: 0.260\n",
      "1 8.5493563162875\n",
      "-----------Validation----------------------------\n",
      "1 8.531878313757662\n",
      "-----------Training----------------------------\n",
      "[2,  2000] loss: 0.262\n",
      "[2,  4000] loss: 0.262\n",
      "[2,  6000] loss: 0.264\n",
      "[2,  8000] loss: 0.264\n",
      "[2, 10000] loss: 0.265\n",
      "2 8.430997454563236\n",
      "-----------Validation----------------------------\n",
      "2 8.469251475365283\n",
      "-----------Training----------------------------\n",
      "[3,  2000] loss: 0.260\n",
      "[3,  4000] loss: 0.265\n",
      "[3,  6000] loss: 0.260\n",
      "[3,  8000] loss: 0.265\n",
      "[3, 10000] loss: 0.263\n",
      "3 8.41225038568449\n",
      "-----------Validation----------------------------\n",
      "3 8.473161280835821\n",
      "-----------Training----------------------------\n",
      "[4,  2000] loss: 0.261\n",
      "[4,  4000] loss: 0.267\n",
      "[4,  6000] loss: 0.262\n",
      "[4,  8000] loss: 0.267\n",
      "[4, 10000] loss: 0.257\n",
      "4 8.405464811238572\n",
      "-----------Validation----------------------------\n",
      "4 8.471333934082292\n",
      "-----------Training----------------------------\n",
      "[5,  2000] loss: 0.264\n",
      "[5,  4000] loss: 0.264\n",
      "[5,  6000] loss: 0.263\n",
      "[5,  8000] loss: 0.262\n",
      "[5, 10000] loss: 0.260\n",
      "5 8.406014651246622\n",
      "-----------Validation----------------------------\n",
      "5 8.46516819130538\n",
      "-----------Training----------------------------\n",
      "[6,  2000] loss: 0.265\n",
      "[6,  4000] loss: 0.265\n",
      "[6,  6000] loss: 0.259\n",
      "[6,  8000] loss: 0.265\n",
      "[6, 10000] loss: 0.261\n",
      "6 8.40284652225555\n",
      "-----------Validation----------------------------\n",
      "6 8.465600314478696\n",
      "-----------Training----------------------------\n",
      "[7,  2000] loss: 0.267\n",
      "[7,  4000] loss: 0.263\n",
      "[7,  6000] loss: 0.260\n",
      "[7,  8000] loss: 0.261\n",
      "[7, 10000] loss: 0.263\n",
      "7 8.404190464506344\n",
      "-----------Validation----------------------------\n",
      "7 8.50385169648424\n",
      "-----------Training----------------------------\n",
      "[8,  2000] loss: 0.260\n",
      "[8,  4000] loss: 0.267\n",
      "[8,  6000] loss: 0.266\n",
      "[8,  8000] loss: 0.258\n",
      "[8, 10000] loss: 0.262\n",
      "8 8.403352860312342\n",
      "-----------Validation----------------------------\n",
      "8 8.477872765278155\n",
      "-----------Training----------------------------\n",
      "[9,  2000] loss: 0.263\n",
      "[9,  4000] loss: 0.264\n",
      "[9,  6000] loss: 0.262\n",
      "[9,  8000] loss: 0.264\n",
      "[9, 10000] loss: 0.261\n",
      "9 8.404227868672672\n",
      "-----------Validation----------------------------\n",
      "9 8.477714877824798\n",
      "-----------Training----------------------------\n",
      "[10,  2000] loss: 0.262\n",
      "[10,  4000] loss: 0.266\n",
      "[10,  6000] loss: 0.262\n",
      "[10,  8000] loss: 0.263\n",
      "[10, 10000] loss: 0.261\n",
      "10 8.398645667748657\n",
      "-----------Validation----------------------------\n",
      "10 8.472245661119459\n",
      "-----------Training----------------------------\n",
      "[11,  2000] loss: 0.258\n",
      "[11,  4000] loss: 0.270\n",
      "[11,  6000] loss: 0.267\n",
      "[11,  8000] loss: 0.257\n",
      "[11, 10000] loss: 0.262\n",
      "11 8.401701761405754\n",
      "-----------Validation----------------------------\n",
      "11 8.468242452253525\n",
      "-----------Training----------------------------\n",
      "[12,  2000] loss: 0.262\n",
      "[12,  4000] loss: 0.262\n",
      "[12,  6000] loss: 0.261\n",
      "[12,  8000] loss: 0.261\n",
      "[12, 10000] loss: 0.266\n",
      "12 8.398377063441979\n",
      "-----------Validation----------------------------\n",
      "12 8.481818867060333\n",
      "-----------Training----------------------------\n",
      "[13,  2000] loss: 0.261\n",
      "[13,  4000] loss: 0.263\n",
      "[13,  6000] loss: 0.260\n",
      "[13,  8000] loss: 0.262\n",
      "[13, 10000] loss: 0.270\n",
      "13 8.396660063607353\n",
      "-----------Validation----------------------------\n",
      "13 8.471955055416505\n",
      "-----------Training----------------------------\n",
      "[14,  2000] loss: 0.265\n",
      "[14,  4000] loss: 0.262\n",
      "[14,  6000] loss: 0.262\n",
      "[14,  8000] loss: 0.262\n",
      "[14, 10000] loss: 0.259\n",
      "14 8.395727385231156\n",
      "-----------Validation----------------------------\n",
      "14 8.473664701373892\n",
      "-----------Training----------------------------\n",
      "[15,  2000] loss: 0.261\n",
      "[15,  4000] loss: 0.262\n",
      "[15,  6000] loss: 0.260\n",
      "[15,  8000] loss: 0.261\n",
      "[15, 10000] loss: 0.265\n",
      "15 8.391521542975152\n",
      "-----------Validation----------------------------\n",
      "15 8.471035869339357\n",
      "-----------Training----------------------------\n",
      "[16,  2000] loss: 0.260\n",
      "[16,  4000] loss: 0.264\n",
      "[16,  6000] loss: 0.264\n",
      "[16,  8000] loss: 0.260\n",
      "[16, 10000] loss: 0.262\n",
      "16 8.390395643511056\n",
      "-----------Validation----------------------------\n",
      "16 8.497891523320757\n",
      "-----------Training----------------------------\n",
      "[17,  2000] loss: 0.262\n",
      "[17,  4000] loss: 0.261\n",
      "[17,  6000] loss: 0.263\n",
      "[17,  8000] loss: 0.265\n",
      "[17, 10000] loss: 0.259\n",
      "17 8.39587312196658\n",
      "-----------Validation----------------------------\n",
      "17 8.492369418529274\n",
      "-----------Training----------------------------\n",
      "[18,  2000] loss: 0.261\n",
      "[18,  4000] loss: 0.261\n",
      "[18,  6000] loss: 0.262\n",
      "[18,  8000] loss: 0.262\n",
      "[18, 10000] loss: 0.263\n",
      "18 8.386354548806777\n",
      "-----------Validation----------------------------\n",
      "18 8.487997022869147\n",
      "-----------Training----------------------------\n",
      "[19,  2000] loss: 0.264\n",
      "[19,  4000] loss: 0.264\n",
      "[19,  6000] loss: 0.259\n",
      "[19,  8000] loss: 0.261\n",
      "[19, 10000] loss: 0.260\n",
      "19 8.379661289742744\n",
      "-----------Validation----------------------------\n",
      "19 8.50118862863077\n",
      "-----------Training----------------------------\n",
      "[20,  2000] loss: 0.265\n",
      "[20,  4000] loss: 0.259\n",
      "[20,  6000] loss: 0.263\n",
      "[20,  8000] loss: 0.262\n",
      "[20, 10000] loss: 0.257\n",
      "20 8.381607645168867\n",
      "-----------Validation----------------------------\n",
      "20 8.50513450210674\n",
      "-----------Training----------------------------\n",
      "[21,  2000] loss: 0.264\n",
      "[21,  4000] loss: 0.256\n",
      "[21,  6000] loss: 0.263\n",
      "[21,  8000] loss: 0.262\n",
      "[21, 10000] loss: 0.261\n",
      "21 8.373158690221185\n",
      "-----------Validation----------------------------\n",
      "21 8.555264878234116\n",
      "-----------Training----------------------------\n",
      "[22,  2000] loss: 0.259\n",
      "[22,  4000] loss: 0.258\n",
      "[22,  6000] loss: 0.261\n",
      "[22,  8000] loss: 0.261\n",
      "[22, 10000] loss: 0.269\n",
      "22 8.371124257371\n",
      "-----------Validation----------------------------\n",
      "22 8.523293189181784\n",
      "-----------Training----------------------------\n",
      "[23,  2000] loss: 0.259\n",
      "[23,  4000] loss: 0.265\n",
      "[23,  6000] loss: 0.259\n",
      "[23,  8000] loss: 0.261\n",
      "[23, 10000] loss: 0.264\n",
      "23 8.368658149226182\n",
      "-----------Validation----------------------------\n",
      "23 8.503563754418545\n",
      "-----------Training----------------------------\n",
      "[24,  2000] loss: 0.268\n",
      "[24,  4000] loss: 0.261\n",
      "[24,  6000] loss: 0.261\n",
      "[24,  8000] loss: 0.259\n",
      "[24, 10000] loss: 0.257\n",
      "24 8.366544106498598\n",
      "-----------Validation----------------------------\n",
      "24 8.513067077948646\n",
      "-----------Training----------------------------\n",
      "[25,  2000] loss: 0.265\n",
      "[25,  4000] loss: 0.261\n",
      "[25,  6000] loss: 0.262\n",
      "[25,  8000] loss: 0.259\n",
      "[25, 10000] loss: 0.261\n",
      "25 8.366828825122374\n",
      "-----------Validation----------------------------\n",
      "25 8.512128222338722\n",
      "-----------Training----------------------------\n",
      "[26,  2000] loss: 0.262\n",
      "[26,  4000] loss: 0.263\n",
      "[26,  6000] loss: 0.259\n",
      "[26,  8000] loss: 0.265\n",
      "[26, 10000] loss: 0.257\n",
      "26 8.353594073319382\n",
      "-----------Validation----------------------------\n",
      "26 8.538140897552497\n",
      "-----------Training----------------------------\n",
      "[27,  2000] loss: 0.261\n",
      "[27,  4000] loss: 0.261\n",
      "[27,  6000] loss: 0.261\n",
      "[27,  8000] loss: 0.261\n",
      "[27, 10000] loss: 0.260\n",
      "27 8.35386506916444\n",
      "-----------Validation----------------------------\n",
      "27 8.528990556326425\n",
      "-----------Training----------------------------\n",
      "[28,  2000] loss: 0.259\n",
      "[28,  4000] loss: 0.261\n",
      "[28,  6000] loss: 0.258\n",
      "[28,  8000] loss: 0.264\n",
      "[28, 10000] loss: 0.262\n",
      "28 8.349150241551216\n",
      "-----------Validation----------------------------\n",
      "28 8.5105131012775\n",
      "-----------Training----------------------------\n",
      "[29,  2000] loss: 0.262\n",
      "[29,  4000] loss: 0.262\n",
      "[29,  6000] loss: 0.260\n",
      "[29,  8000] loss: 0.259\n",
      "[29, 10000] loss: 0.262\n",
      "29 8.3411662991301\n",
      "-----------Validation----------------------------\n",
      "29 8.553725177662214\n",
      "-----------Training----------------------------\n",
      "[30,  2000] loss: 0.260\n",
      "[30,  4000] loss: 0.262\n",
      "[30,  6000] loss: 0.263\n",
      "[30,  8000] loss: 0.262\n",
      "[30, 10000] loss: 0.257\n",
      "30 8.34207379187586\n",
      "-----------Validation----------------------------\n",
      "30 8.546279309527131\n",
      "-----------Training----------------------------\n",
      "[31,  2000] loss: 0.257\n",
      "[31,  4000] loss: 0.261\n",
      "[31,  6000] loss: 0.262\n",
      "[31,  8000] loss: 0.260\n",
      "[31, 10000] loss: 0.261\n",
      "31 8.334354331574472\n",
      "-----------Validation----------------------------\n",
      "31 8.529562058981636\n",
      "-----------Training----------------------------\n",
      "[32,  2000] loss: 0.262\n",
      "[32,  4000] loss: 0.256\n",
      "[32,  6000] loss: 0.262\n",
      "[32,  8000] loss: 0.262\n",
      "[32, 10000] loss: 0.262\n",
      "32 8.340498450478188\n",
      "-----------Validation----------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 8.563760453394536\n",
      "-----------Training----------------------------\n",
      "[33,  2000] loss: 0.258\n",
      "[33,  4000] loss: 0.260\n",
      "[33,  6000] loss: 0.258\n",
      "[33,  8000] loss: 0.263\n",
      "[33, 10000] loss: 0.260\n",
      "33 8.329866844953592\n",
      "-----------Validation----------------------------\n",
      "33 8.620896912612883\n",
      "-----------Training----------------------------\n",
      "[34,  2000] loss: 0.260\n",
      "[34,  4000] loss: 0.261\n",
      "[34,  6000] loss: 0.261\n",
      "[34,  8000] loss: 0.262\n",
      "[34, 10000] loss: 0.258\n",
      "34 8.324492848433064\n",
      "-----------Validation----------------------------\n",
      "34 8.598546018899945\n",
      "-----------Training----------------------------\n",
      "[35,  2000] loss: 0.264\n",
      "[35,  4000] loss: 0.257\n",
      "[35,  6000] loss: 0.255\n",
      "[35,  8000] loss: 0.260\n",
      "[35, 10000] loss: 0.263\n",
      "35 8.315586968636026\n",
      "-----------Validation----------------------------\n",
      "35 8.538482253742062\n",
      "-----------Training----------------------------\n",
      "[36,  2000] loss: 0.262\n",
      "[36,  4000] loss: 0.260\n",
      "[36,  6000] loss: 0.261\n",
      "[36,  8000] loss: 0.261\n",
      "[36, 10000] loss: 0.260\n",
      "36 8.324623722165079\n",
      "-----------Validation----------------------------\n",
      "36 8.588910873134607\n",
      "-----------Training----------------------------\n",
      "[37,  2000] loss: 0.258\n",
      "[37,  4000] loss: 0.263\n",
      "[37,  6000] loss: 0.263\n",
      "[37,  8000] loss: 0.258\n",
      "[37, 10000] loss: 0.260\n",
      "37 8.310729704962837\n",
      "-----------Validation----------------------------\n",
      "37 8.63255860045139\n",
      "-----------Training----------------------------\n",
      "[38,  2000] loss: 0.259\n",
      "[38,  4000] loss: 0.261\n",
      "[38,  6000] loss: 0.258\n",
      "[38,  8000] loss: 0.258\n",
      "[38, 10000] loss: 0.261\n",
      "38 8.312105435849316\n",
      "-----------Validation----------------------------\n",
      "38 8.61740702624033\n",
      "-----------Training----------------------------\n",
      "[39,  2000] loss: 0.261\n",
      "[39,  4000] loss: 0.256\n",
      "[39,  6000] loss: 0.262\n",
      "[39,  8000] loss: 0.259\n",
      "[39, 10000] loss: 0.262\n",
      "39 8.303294482717709\n",
      "-----------Validation----------------------------\n",
      "39 8.651448035220726\n",
      "-----------Training----------------------------\n",
      "[40,  2000] loss: 0.257\n",
      "[40,  4000] loss: 0.256\n",
      "[40,  6000] loss: 0.264\n",
      "[40,  8000] loss: 0.262\n",
      "[40, 10000] loss: 0.259\n",
      "40 8.30078576191753\n",
      "-----------Validation----------------------------\n",
      "40 8.64515708369882\n",
      "-----------Training----------------------------\n",
      "[41,  2000] loss: 0.258\n",
      "[41,  4000] loss: 0.260\n",
      "[41,  6000] loss: 0.256\n",
      "[41,  8000] loss: 0.262\n",
      "[41, 10000] loss: 0.259\n",
      "41 8.294609878663303\n",
      "-----------Validation----------------------------\n",
      "41 8.631415262016153\n",
      "-----------Training----------------------------\n",
      "[42,  2000] loss: 0.262\n",
      "[42,  4000] loss: 0.257\n",
      "[42,  6000] loss: 0.259\n",
      "[42,  8000] loss: 0.258\n",
      "[42, 10000] loss: 0.257\n",
      "42 8.289663339731645\n",
      "-----------Validation----------------------------\n",
      "42 8.706386604277967\n",
      "-----------Training----------------------------\n",
      "[43,  2000] loss: 0.261\n",
      "[43,  4000] loss: 0.259\n",
      "[43,  6000] loss: 0.255\n",
      "[43,  8000] loss: 0.260\n",
      "[43, 10000] loss: 0.260\n",
      "43 8.28925023426004\n",
      "-----------Validation----------------------------\n",
      "43 8.654071562247408\n",
      "-----------Training----------------------------\n",
      "[44,  2000] loss: 0.259\n",
      "[44,  4000] loss: 0.255\n",
      "[44,  6000] loss: 0.261\n",
      "[44,  8000] loss: 0.258\n",
      "[44, 10000] loss: 0.261\n",
      "44 8.282632970053053\n",
      "-----------Validation----------------------------\n",
      "44 8.698459108735376\n",
      "-----------Training----------------------------\n",
      "[45,  2000] loss: 0.256\n",
      "[45,  4000] loss: 0.257\n",
      "[45,  6000] loss: 0.261\n",
      "[45,  8000] loss: 0.257\n",
      "[45, 10000] loss: 0.263\n",
      "45 8.276424840550844\n",
      "-----------Validation----------------------------\n",
      "45 8.732549470167175\n",
      "-----------Training----------------------------\n",
      "[46,  2000] loss: 0.255\n",
      "[46,  4000] loss: 0.253\n",
      "[46,  6000] loss: 0.261\n",
      "[46,  8000] loss: 0.260\n",
      "[46, 10000] loss: 0.263\n",
      "46 8.267051316778135\n",
      "-----------Validation----------------------------\n",
      "46 8.774701789191653\n",
      "-----------Training----------------------------\n",
      "[47,  2000] loss: 0.260\n",
      "[47,  4000] loss: 0.255\n",
      "[47,  6000] loss: 0.261\n",
      "[47,  8000] loss: 0.257\n",
      "[47, 10000] loss: 0.260\n",
      "47 8.256592882095822\n",
      "-----------Validation----------------------------\n",
      "47 8.74837222398786\n",
      "-----------Training----------------------------\n",
      "[48,  2000] loss: 0.261\n",
      "[48,  4000] loss: 0.259\n",
      "[48,  6000] loss: 0.253\n",
      "[48,  8000] loss: 0.255\n",
      "[48, 10000] loss: 0.261\n",
      "48 8.27076407300642\n",
      "-----------Validation----------------------------\n",
      "48 8.697933186520176\n",
      "-----------Training----------------------------\n",
      "[49,  2000] loss: 0.258\n",
      "[49,  4000] loss: 0.257\n",
      "[49,  6000] loss: 0.263\n",
      "[49,  8000] loss: 0.259\n",
      "[49, 10000] loss: 0.255\n",
      "49 8.2529376805991\n",
      "-----------Validation----------------------------\n",
      "49 8.724227834875782\n",
      "-----------Training----------------------------\n",
      "[50,  2000] loss: 0.258\n",
      "[50,  4000] loss: 0.255\n",
      "[50,  6000] loss: 0.259\n",
      "[50,  8000] loss: 0.260\n",
      "[50, 10000] loss: 0.258\n",
      "50 8.254261122896017\n",
      "-----------Validation----------------------------\n",
      "50 8.742937453127608\n"
     ]
    }
   ],
   "source": [
    "# Train the Model   \n",
    "for epoch in range(num_epochs):\n",
    "    print('-----------Training----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (star, labels) in enumerate(train_loader):  \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        #print(star[:,49])\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(star)\n",
    "        #print(softmax(outputs))\n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        if regularization:\n",
    "            l1_regularization = 0\n",
    "            l2_regularization = 0\n",
    "            lambda1 = 0.001\n",
    "            lambda2 = 0.001\n",
    "            for param in net.parameters():\n",
    "                l1_regularization += torch.norm(param, 1)**2\n",
    "                l2_regularization += torch.norm(param, 2)**2\n",
    "            loss = loss + lambda1*l1_regularization + lambda2*l2_regularization\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += outputs.shape[0] * loss.item()      \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    print(epoch+1, epoch_loss / len(train_loader))\n",
    "    print('-----------Validation----------------------------')\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (star, labels) in enumerate(val_loader):  \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  # zero the gradient buffer\n",
    "        outputs = net(star)\n",
    "        loss = criterion(outputs, labels.long())    \n",
    "        epoch_loss += outputs.shape[0] * loss.item()      \n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "    print(epoch+1, epoch_loss / len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in test_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on test objects: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in train_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on train objects: %d %%' % (100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/pandas/compat/_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.8' currently installed).\n",
      "  warnings.warn(msg, UserWarning)\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/franciscoperez/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq1_harmonics_rel_phase_0\n",
      "Freq2_harmonics_rel_phase_0\n",
      "Freq3_harmonics_rel_phase_0\n",
      "downsampling\n",
      "28847\n",
      "362654\n",
      "done downsampling\n",
      "        Amplitude  AndersonDarling  Autocor_length  Beyond1Std    CAR_mean  \\\n",
      "408027     0.0135         0.171236               2    0.330189   25.439073   \n",
      "375271     0.3820         0.000000               1    0.395349   31.547271   \n",
      "389707     0.8450         0.000000              12    0.401869    0.020022   \n",
      "164826     0.2120         0.020923               1    0.330000  100.205777   \n",
      "326806     0.0910         0.000000              26    0.374150    0.074815   \n",
      "...           ...              ...             ...         ...         ...   \n",
      "172801     0.2985         0.001336               1    0.310000   15.014133   \n",
      "158152     0.3230         0.020765               1    0.319277   31.108072   \n",
      "165728     0.3185         0.000001               1    0.387960   55.873805   \n",
      "371704     0.2815         0.012085               1    0.373832    9.926920   \n",
      "427410     0.2585         0.000000               9    0.136667    7.080861   \n",
      "\n",
      "        CAR_sigma     CAR_tau       Con       Eta_e  FluxPercentileRatioMid20  \\\n",
      "408027   0.676541    0.531677  0.000000   12.423927                  0.227273   \n",
      "375271   0.509930    0.521918  0.000000   17.040413                  0.192201   \n",
      "389707   0.029755  634.420295  0.000000    0.036964                  0.177397   \n",
      "164826   0.274467    0.190530  0.000000   10.623095                  0.168022   \n",
      "326806   0.007373  192.549098  0.000000    4.013749                  0.182857   \n",
      "...           ...         ...       ...         ...                       ...   \n",
      "172801   0.179187    1.257823  0.000000    9.600625                  0.119284   \n",
      "158152   1.055467    0.607654  0.000000  304.548968                  0.171540   \n",
      "165728   0.638156    0.343611  0.000000  946.516425                  0.196007   \n",
      "371704   0.184253    1.615182  0.000000   82.256479                  0.154982   \n",
      "427410   0.118943    2.238932  0.050336   38.468208                  0.120930   \n",
      "\n",
      "        ...      Skew  SlottedA_length  SmallKurtosis       Std  StetsonK  \\\n",
      "408027  ... -0.262801          1.90564      -0.119342  0.007046  0.832769   \n",
      "375271  ... -0.770261          1.88706      -0.409288  0.224662  0.836869   \n",
      "389707  ...  0.190621         32.45196      -1.049980  0.506747  0.868600   \n",
      "164826  ...  0.345508          1.93906      -0.051019  0.115483  0.825544   \n",
      "326806  ... -0.605585          1.24836      -1.139705  0.061183  0.889249   \n",
      "...     ...       ...              ...            ...       ...       ...   \n",
      "172801  ... -0.446165          1.90777       0.270409  0.150675  0.778933   \n",
      "158152  ... -0.425599          0.11180       0.104516  0.158940  0.790424   \n",
      "165728  ... -0.382922          0.08874      -0.684252  0.177684  0.828207   \n",
      "371704  ... -0.252099          0.91483      -0.829988  0.160649  0.838638   \n",
      "427410  ...  2.057395          0.23892       3.863118  0.135535  0.735684   \n",
      "\n",
      "        StetsonK_AC  StructureFunction_index_21  StructureFunction_index_31  \\\n",
      "408027     0.749718                    1.694013                    2.260294   \n",
      "375271     0.740809                    1.466906                    1.742868   \n",
      "389707     0.881887                    1.926455                    2.802499   \n",
      "164826     0.753878                    1.697313                    2.239315   \n",
      "326806     0.837623                    1.820805                    2.541535   \n",
      "...             ...                         ...                         ...   \n",
      "172801     0.754054                    1.869365                    2.682081   \n",
      "158152     0.823441                    1.892922                    2.877066   \n",
      "165728     0.860433                    1.493950                    1.764119   \n",
      "371704     0.803060                    1.302210                    1.301092   \n",
      "427410     0.895719                    1.509681                    1.747719   \n",
      "\n",
      "        StructureFunction_index_32   label  \n",
      "408027                    1.457165  ClassB  \n",
      "375271                    1.256151  ClassA  \n",
      "389707                    1.458818  ClassB  \n",
      "164826                    1.345530  ClassA  \n",
      "326806                    1.400537  ClassB  \n",
      "...                            ...     ...  \n",
      "172801                    1.492602  ClassA  \n",
      "158152                    1.589674  ClassA  \n",
      "165728                    1.327828  ClassA  \n",
      "371704                    1.322387  ClassA  \n",
      "427410                    1.227681  ClassB  \n",
      "\n",
      "[10000 rows x 61 columns]\n",
      "(7824, 61)\n",
      "(28625, 60)\n",
      "(28625, 61)\n",
      "Index(['PeriodLS', 'Amplitude', 'AndersonDarling', 'Autocor_length',\n",
      "       'Beyond1Std', 'CAR_mean', 'CAR_sigma', 'CAR_tau', 'Con', 'Eta_e',\n",
      "       'FluxPercentileRatioMid20', 'FluxPercentileRatioMid35',\n",
      "       'FluxPercentileRatioMid50', 'FluxPercentileRatioMid65',\n",
      "       'FluxPercentileRatioMid80', 'Freq1_harmonics_amplitude_0',\n",
      "       'Freq1_harmonics_amplitude_1', 'Freq1_harmonics_amplitude_2',\n",
      "       'Freq1_harmonics_amplitude_3', 'Freq1_harmonics_rel_phase_1',\n",
      "       'Freq1_harmonics_rel_phase_2', 'Freq1_harmonics_rel_phase_3',\n",
      "       'Freq2_harmonics_amplitude_0', 'Freq2_harmonics_amplitude_1',\n",
      "       'Freq2_harmonics_amplitude_2', 'Freq2_harmonics_amplitude_3',\n",
      "       'Freq2_harmonics_rel_phase_1', 'Freq2_harmonics_rel_phase_2',\n",
      "       'Freq2_harmonics_rel_phase_3', 'Freq3_harmonics_amplitude_0',\n",
      "       'Freq3_harmonics_amplitude_1', 'Freq3_harmonics_amplitude_2',\n",
      "       'Freq3_harmonics_amplitude_3', 'Freq3_harmonics_rel_phase_1',\n",
      "       'Freq3_harmonics_rel_phase_2', 'Freq3_harmonics_rel_phase_3', 'Gskew',\n",
      "       'LinearTrend', 'MaxSlope', 'Mean', 'Meanvariance', 'MedianAbsDev',\n",
      "       'MedianBRP', 'PairSlopeTrend', 'PercentAmplitude',\n",
      "       'PercentDifferenceFluxPercentile', 'Period_fit', 'Psi_CS', 'Psi_eta',\n",
      "       'Q31', 'Rcs', 'Skew', 'SlottedA_length', 'SmallKurtosis', 'Std',\n",
      "       'StetsonK', 'StetsonK_AC', 'StructureFunction_index_21',\n",
      "       'StructureFunction_index_31', 'StructureFunction_index_32', 'label'],\n",
      "      dtype='object')\n",
      "____get_tensor_function____\n",
      "shape tensor:  torch.Size([4800, 60])\n",
      "____get_tensor_function____\n",
      "shape tensor:  torch.Size([1201, 60])\n",
      "____get_tensor_function____\n",
      "shape tensor:  torch.Size([6259, 60])\n",
      "____get_tensor_function____\n",
      "shape tensor:  torch.Size([7824, 60])\n",
      "____get_tensor_function____\n",
      "shape tensor:  torch.Size([1201, 60])\n",
      "____get_tensor_function____\n",
      "shape tensor:  torch.Size([28625, 60])\n",
      "____get_tensor_function____\n",
      "shape tensor:  torch.Size([28625, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/franciscoperez/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(loc, value, pi)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  1000\n",
      "Epoch:  0\n",
      "training: epoch:  1  loss:  13.874267727136612 -- aux loss:  281.3954516351223\n",
      "validating: epoch:  1  loss:  2.4256163835525513\n",
      "Epoch:  1\n",
      "training: epoch:  2  loss:  9.654902130365372 -- aux loss:  194.12608447670937\n",
      "validating: epoch:  2  loss:  1.8253071308135986\n",
      "Epoch:  2\n",
      "training: epoch:  3  loss:  8.884406059980392 -- aux loss:  164.31752794981003\n",
      "validating: epoch:  3  loss:  1.6738594472408295\n",
      "Epoch:  3\n",
      "training: epoch:  4  loss:  8.580181121826172 -- aux loss:  156.16178131103516\n",
      "validating: epoch:  4  loss:  1.6230806410312653\n",
      "Epoch:  4\n",
      "training: epoch:  5  loss:  8.407962620258331 -- aux loss:  152.9868836402893\n",
      "validating: epoch:  5  loss:  1.6000154912471771\n",
      "Epoch:  5\n",
      "training: epoch:  6  loss:  8.32447761297226 -- aux loss:  151.421773314476\n",
      "validating: epoch:  6  loss:  1.5878401100635529\n",
      "Epoch:  6\n",
      "training: epoch:  7  loss:  8.274911493062973 -- aux loss:  150.5534763634205\n",
      "validating: epoch:  7  loss:  1.5807642340660095\n",
      "Epoch:  7\n",
      "training: epoch:  8  loss:  8.24170857667923 -- aux loss:  150.0303121805191\n",
      "validating: epoch:  8  loss:  1.5763652324676514\n",
      "Epoch:  8\n",
      "training: epoch:  9  loss:  8.217388242483139 -- aux loss:  149.69663602113724\n",
      "validating: epoch:  9  loss:  1.5734906494617462\n",
      "Epoch:  9\n",
      "training: epoch:  10  loss:  8.198898792266846 -- aux loss:  149.4741425216198\n",
      "validating: epoch:  10  loss:  1.5715355575084686\n",
      "The current loss: 1.5715355575084686\n",
      "the_last_loss: 1.5734906494617462\n",
      "trigger times: 0\n",
      "recall\n",
      "tensor(0.9969)\n",
      "precision\n",
      "tensor(0.9807)\n",
      "f1_score\n",
      "tensor(0.9888)\n",
      "Accuracy of the network on test objects: 98 %\n",
      "98.07216\n",
      "recall\n",
      "tensor(1.)\n",
      "precision\n",
      "tensor(1.)\n",
      "f1_score\n",
      "tensor(1.)\n",
      "Accuracy of the network on test objects: 100 %\n",
      "100.0\n",
      "recall\n",
      "tensor(0.9518)\n",
      "precision\n",
      "tensor(0.6101)\n",
      "f1_score\n",
      "tensor(0.7436)\n",
      "Accuracy of the network on test objects: 61 %\n",
      "61.00905\n",
      "sum mask2 - L1:  tensor(5737, device='cuda:0')\n",
      "sum mask2 - L2:  tensor(9508, device='cuda:0')\n",
      "sum mask2 - L3:  tensor(192, device='cuda:0')\n",
      "sum mask1 - L1 (aux):  tensor(263, device='cuda:0')\n",
      "sum mask1 - L2 (aux):  tensor(492, device='cuda:0')\n",
      "sum mask1 - L3 (aux):  tensor(8, device='cuda:0')\n",
      "Epoch:  10\n",
      "training: epoch:  11  loss:  8.18438670039177 -- aux loss:  149.3201884329319\n",
      "validating: epoch:  11  loss:  1.5701650083065033\n",
      "Epoch:  11\n",
      "training: epoch:  12  loss:  8.172481536865234 -- aux loss:  149.2110240459442\n",
      "validating: epoch:  12  loss:  1.5691837072372437\n",
      "Epoch:  12\n",
      "training: epoch:  13  loss:  8.162314265966415 -- aux loss:  149.1321197450161\n",
      "validating: epoch:  13  loss:  1.5684691369533539\n",
      "Epoch:  13\n",
      "training: epoch:  14  loss:  8.153753787279129 -- aux loss:  149.07403698563576\n",
      "validating: epoch:  14  loss:  1.5679041147232056\n",
      "Epoch:  14\n",
      "training: epoch:  15  loss:  8.156382501125336 -- aux loss:  149.0151670873165\n",
      "validating: epoch:  15  loss:  1.5673144161701202\n",
      "Epoch:  15\n",
      "training: epoch:  16  loss:  8.147990316152573 -- aux loss:  148.97630098462105\n",
      "validating: epoch:  16  loss:  1.5670154690742493\n",
      "Epoch:  16\n",
      "training: epoch:  17  loss:  8.13840800523758 -- aux loss:  148.95368787646294\n",
      "validating: epoch:  17  loss:  1.5668213367462158\n",
      "Epoch:  17\n",
      "training: epoch:  18  loss:  8.130644410848618 -- aux loss:  148.93849152326584\n",
      "validating: epoch:  18  loss:  1.5666871964931488\n",
      "Epoch:  18\n",
      "training: epoch:  19  loss:  8.123696953058243 -- aux loss:  148.92781990766525\n",
      "validating: epoch:  19  loss:  1.5665915608406067\n",
      "Epoch:  19\n",
      "training: epoch:  20  loss:  8.1171133518219 -- aux loss:  148.9201264679432\n",
      "validating: epoch:  20  loss:  1.5665219128131866\n",
      "The current loss: 1.5665219128131866\n",
      "the_last_loss: 1.5665915608406067\n",
      "trigger times: 0\n",
      "recall\n",
      "tensor(0.9978)\n",
      "precision\n",
      "tensor(0.9843)\n",
      "f1_score\n",
      "tensor(0.9910)\n",
      "Accuracy of the network on test objects: 98 %\n",
      "98.42585\n",
      "recall\n",
      "tensor(1.)\n",
      "precision\n",
      "tensor(1.)\n",
      "f1_score\n",
      "tensor(1.)\n",
      "Accuracy of the network on test objects: 100 %\n",
      "100.0\n",
      "recall\n",
      "tensor(0.9497)\n",
      "precision\n",
      "tensor(0.6272)\n",
      "f1_score\n",
      "tensor(0.7555)\n",
      "Accuracy of the network on test objects: 62 %\n",
      "62.723553\n",
      "sum mask2 - L1:  tensor(5737, device='cuda:0')\n",
      "sum mask2 - L2:  tensor(9508, device='cuda:0')\n",
      "sum mask2 - L3:  tensor(192, device='cuda:0')\n",
      "sum mask1 - L1 (aux):  tensor(263, device='cuda:0')\n",
      "sum mask1 - L2 (aux):  tensor(492, device='cuda:0')\n",
      "sum mask1 - L3 (aux):  tensor(8, device='cuda:0')\n",
      "Epoch:  20\n",
      "training: epoch:  21  loss:  8.110187381505966 -- aux loss:  148.91448578238487\n",
      "validating: epoch:  21  loss:  1.5664705336093903\n",
      "Epoch:  21\n",
      "training: epoch:  22  loss:  8.102767050266266 -- aux loss:  148.91028958559036\n",
      "validating: epoch:  22  loss:  1.5664319396018982\n",
      "Epoch:  22\n",
      "training: epoch:  23  loss:  8.095290750265121 -- aux loss:  148.9071336388588\n",
      "validating: epoch:  23  loss:  1.5664028525352478\n",
      "Epoch:  23\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as f \n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random \n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from scipy import stats\n",
    "from itertools import cycle\n",
    "import sys\n",
    "import utilities as ut\n",
    "from Network import Net\n",
    "import Network as nn\n",
    "SEED = 0\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "results = []\n",
    "num_classes = 2\n",
    "\n",
    "learning_rate = 0.001\n",
    "samples = 3000\n",
    "\n",
    "for epsilon in [0.2]:\n",
    "    for batch_size in [256]:\n",
    "        for hidden_size in [100]:\n",
    "            for EPS1 in [0.005]:\n",
    "                for n in [10000]:\n",
    "                    for aux_loss_activated in [True]:\n",
    "                        for opt in [2]:\n",
    "                            for t in range(10):\n",
    "                                train_dataset, test_dataset = ut.load_files(dataset=1)\n",
    "                                input_size = train_dataset.shape[1]-1                                \n",
    "                                if n < 50000:\n",
    "                                    train_dataset = ut.down_sampling(train_dataset)\n",
    "                                    train_dataset = train_dataset.sample(n)\n",
    "                                    print(train_dataset)\n",
    "                                else: \n",
    "                                    trainig_dataset_a = train_dataset[train_dataset.label=='ClassA']\n",
    "                                    print('shape: ', trainig_dataset_a.shape[0])\n",
    "                                    n2 = n - trainig_dataset_a.shape[0]\n",
    "                                    print('clase no RR Lrae', n2)\n",
    "                                    trainig_dataset_b = train_dataset[~(train_dataset.label=='ClassA')].sample(n2)\n",
    "                                    train_dataset = pd.concat([trainig_dataset_a, trainig_dataset_b])\n",
    "                                \n",
    "                                train_dataset, _ = ut.delete_outliers(train_dataset, test_dataset)\n",
    "\n",
    "                                train_dataset = ut.sort_columns(train_dataset)\n",
    "                                test_dataset = ut.sort_columns(test_dataset)\n",
    "                                #... normalize ...\n",
    "                                train_dataset, test_dataset = ut.normalize(train_dataset, test_dataset)\n",
    "                                #print(train_dataset.columns)\n",
    "                                test_dataset_pred = test_dataset.copy()\n",
    "                                train_dataset_pred = train_dataset.copy()\n",
    "                                try:\n",
    "                                    data_prior = ut.generate_samples(samples, train_dataset, epsilon,  option = opt)\n",
    "\n",
    "                                    train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.2, random_state=42)\n",
    "\n",
    "                                    train_dataset_prior, val_dataset_prior = train_test_split(data_prior, test_size=0.2, random_state=42)\n",
    "                                    print(train_dataset_prior.columns)\n",
    "\n",
    "\n",
    "                                    _, _, train_target_prior, train_loader_prior = ut.get_tensors(train_dataset_prior, batch_size)\n",
    "                                    _, _, val_target_prior, val_loader_prior     = ut.get_tensors(val_dataset_prior, batch_size)\n",
    "                                    _, _, train_target, train_loader             = ut.get_tensors(train_dataset, batch_size)\n",
    "                                    _, _, train_target_pred, train_loader_pred   = ut.get_tensors(train_dataset_pred, batch_size)\n",
    "                                    _, _, val_target, val_loader                 = ut.get_tensors(val_dataset_prior, batch_size)\n",
    "                                    _, _, test_target, test_loader               = ut.get_tensors(test_dataset, batch_size)\n",
    "                                    _, _, test_target_pred, test_loader_pred     = ut.get_tensors(test_dataset_pred, batch_size)\n",
    "\n",
    "                                    net = Net(input_size, hidden_size, hidden_size, num_classes)\n",
    "                                    net.cuda()\n",
    "\n",
    "                                    hist_val, hist_train = nn.train(net, train_loader, train_loader_prior, val_loader, test_loader,\n",
    "                                    EPS1, learning_rate, input_size, aux_loss_activated=aux_loss_activated)\n",
    "\n",
    "                                    acc_train, recall_train, f1_train = nn.get_results(net, train_loader, input_size)\n",
    "                                    acc_test, recall_test, f1_test  = nn.get_results(net, test_loader, input_size)\n",
    "                                    results.append([acc_train, acc_test,recall_train, recall_test, f1_train, f1_test, epsilon, batch_size, hidden_size, aux_loss_activated, EPS1, n, opt])\n",
    "                                    #pd.DataFrame(results, columns=['acc_train', 'acc_test','recall_train', 'recall_test','f1_train', 'f1_test',\n",
    "                                    # 'epsilon', 'batch_size', 'hidden_size',\n",
    "                                    # 'aux_loss_activated', 'EPS1', 'n', 'opt']).to_csv('25-04-2022-results1D.csv')\n",
    "                                    nn.get_roc_curve(net, test_loader, input_size, name= str(t)+\"_\", title=\"Regularization\")\n",
    "                                except Exception as error:\n",
    "                                    print(error) \n",
    "                                    print(str(epsilon)+\"-\"+str(batch_size)+\"-\"+str(hidden_size)+\"-\"+str(aux_loss_activated)+\"-\"+str(EPS1))\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.get_roc_curve(net, test_loader, input_size, title=\"Regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.plot_training(hist_val, hist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.get_results(net, train_loader, input_size)\n",
    "nn.get_results(net, test_loader, input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_val, label ='validation')\n",
    "plt.plot(hist_train, label ='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.savefig('images/'+str(samples)+'_'+str(epsilon)+'_'+str(n)+\"_\"+str(hidden_size)+\"_Loss_Training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#csv_file = open(\"size_MLP_noise.csv\", \"a\")\n",
    "#csv_file.write(str(np.asarray(acc_testing))+\",\"+str(np.asarray(acc_training))+\",\"+str(samples)+\",\"+str(epsilon)+\",\"+str(n)+\",\"+str(hidden_size)+\"\\n\")\n",
    "#csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, intermediates, intermediates2, labels = ut.get_representations(net, train_loader, device)\n",
    "outputs_test, intermediates_test, intermediates2_test, labels_test = ut.get_representations(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pca_data, intermediate_pca_data_test = ut.get_pca(intermediates, data_test=intermediates_test)\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "ut.plot_representations(intermediate_pca_data, labels, axs[0, 0])\n",
    "ut.plot_representations(intermediate_pca_data_test, labels_test, axs[1, 0])\n",
    "intermediate2_pca_data, intermediate2_pca_data_test = ut.get_pca(intermediates2, data_test=intermediates2_test)\n",
    "ut.plot_representations(intermediate2_pca_data, labels, axs[0, 1])\n",
    "ut.plot_representations(intermediate2_pca_data_test, labels_test, axs[1, 1])\n",
    "output_pca_data, output_pca_data_test = ut.get_pca(outputs, data_test=outputs_test)\n",
    "ut.plot_representations(output_pca_data, labels, axs[0, 2])\n",
    "ut.plot_representations(output_pca_data_test, labels_test, axs[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "print('first')\n",
    "reducer = umap.UMAP()\n",
    "print('first ..')\n",
    "mapper_train = reducer.fit_transform(intermediates.numpy())\n",
    "print('first ...')\n",
    "reducer = umap.UMAP()\n",
    "mapper_test = reducer.fit_transform(intermediates_test.numpy())\n",
    "ut.plot_representations(mapper_train, labels, axs[0, 0])\n",
    "ut.plot_representations(mapper_test, labels_test, axs[1, 0])\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "mapper_train = reducer.fit_transform(intermediates2.numpy())\n",
    "reducer = umap.UMAP()\n",
    "mapper_test = reducer.fit_transform(intermediates2_test.numpy())\n",
    "ut.plot_representations(mapper_train, labels, axs[0, 1])\n",
    "ut.plot_representations(mapper_test, labels_test, axs[1, 1])\n",
    "\n",
    "reducer = umap.UMAP()\n",
    "mapper_train = reducer.fit_transform(outputs.numpy())\n",
    "reducer = umap.UMAP()\n",
    "mapper_test = reducer.fit_transform(outputs_test.numpy())\n",
    "\n",
    "ut.plot_representations(mapper_train, labels, axs[0, 2])\n",
    "ut.plot_representations(mapper_test, labels_test, axs[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CURVES = 25000\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "intermediate_tsne_data, intermediate_tsne_data_test = ut.get_tsne(intermediates, data_test= intermediates_test, n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate_tsne_data, labels, axs[0, 0],  n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate_tsne_data_test, labels_test, axs[1, 0], n_curves = N_CURVES)\n",
    "\n",
    "intermediate2_tsne_data, intermediate2_tsne_data_test = ut.get_tsne(intermediates2, data_test=intermediates2_test, n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate2_tsne_data, labels, axs[0, 1], n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate2_tsne_data_test, labels_test, axs[1, 1], n_curves = N_CURVES)\n",
    "\n",
    "output_tsne_data, output2_tsne_data_test = ut.get_tsne(outputs, data_test=outputs_test, n_curves = N_CURVES)\n",
    "ut.plot_representations(output_tsne_data, labels, axs[0, 2], n_curves = N_CURVES)\n",
    "ut.plot_representations(output2_tsne_data_test, labels_test, axs[1, 2], n_curves = N_CURVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fig, ax = plt.subplots(1, 1, figsize=(15,15))\n",
    "#curves, labels, probs_train = ut.get_predictions(net, train_loader_pred, device)\n",
    "#pred_labels = probs_train.argmax(1, keepdim = True)\n",
    "#ut.plot_confusion_matrix(np.round(labels), pred_labels, ax)\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "curves, labels, probs_test = ut.get_predictions(net, test_loader_pred, device)\n",
    "pred_labels = probs_test.argmax(1, keepdim = True)\n",
    "ut.plot_confusion_matrix(np.round(labels), pred_labels, ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves, labels, probs_train_sample = ut.get_predictions(net, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_WEIGHTS = 25\n",
    "#weights = net.fc2.weight.data\n",
    "#plot_weights(weights, N_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = net.fc1.weight.data\n",
    "w1 = weights1.cpu().numpy().reshape(-1,1)\n",
    "weights2 = net.fc2.weight.data\n",
    "w2 = weights2.cpu().numpy().reshape(-1,1)\n",
    "weights3 = net.fc3.weight.data\n",
    "w3 = weights3.cpu().numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "ax1.hist(w1, color='black')\n",
    "ax1.set_xlabel('Layer 1')\n",
    "ax2.hist(w2, color='black')\n",
    "ax2.set_xlabel('Layer 2')\n",
    "ax3.hist(w3, color='black')\n",
    "ax3.set_xlabel('Layer 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\n",
    "curves, labels, probs_train = ut.get_predictions(net, train_loader_pred, device)\n",
    "pred_labels = torch.argmax(probs_train, 1)\n",
    "ut.plot_confusion_matrix(np.round(labels), pred_labels, ax1)\n",
    "curves, labels, probs_test = ut.get_predictions(net, test_loader_pred, device)\n",
    "pred_labels = torch.argmax(probs_test, 1)\n",
    "ut.plot_confusion_matrix(np.round(labels), pred_labels, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.functional as f \n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "from sklearn import manifold\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(labels, pred_labels, ax):\n",
    "    #fig = plt.figure(figsize = (10, 10));\n",
    "    #ax = fig.add_subplot(1, 1, 1);\n",
    "    cm = metrics.confusion_matrix(labels, pred_labels, normalize='pred');\n",
    "    cm = metrics.ConfusionMatrixDisplay(cm);\n",
    "    cm.plot(cmap = 'Blues', ax = ax)\n",
    "    cm.im_.colorbar.remove()\n",
    "\n",
    "\n",
    "def get_predictions(model, iterator, device):\n",
    "    model.eval()\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    probs = []\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for (x, y) in iterator:\n",
    "\n",
    "            x = x.to(device)\n",
    "\n",
    "            y_pred, _, _ = model(x)\n",
    "\n",
    "            y_prob = f.softmax(y_pred, dim = -1)\n",
    "            top_pred = y_prob.argmax(1, keepdim = True)\n",
    "\n",
    "            images.append(x.cpu())\n",
    "            labels.append(y.cpu())\n",
    "            probs.append(y_prob.cpu())\n",
    "\n",
    "    images = torch.cat(images, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "    probs = torch.cat(probs, dim = 0)\n",
    "\n",
    "    return images, labels, probs\n",
    "\n",
    "def regularization_method(params):\n",
    "    l1_regularization = 0\n",
    "    l2_regularization = 0\n",
    "    lambda1 = 0.001\n",
    "    lambda2 = 0.001\n",
    "    for param in params:\n",
    "        l1_regularization += torch.norm(param, 1)**2\n",
    "        l2_regularization += torch.norm(param, 2)**2\n",
    "    loss = loss + lambda1*l1_regularization + lambda2*l2_regularization\n",
    "    \n",
    "def plot_weights(weights, n_weights):\n",
    "\n",
    "    rows = int(np.sqrt(n_weights))\n",
    "    cols = int(np.sqrt(n_weights))\n",
    "\n",
    "    fig = plt.figure(figsize = (20, 10))\n",
    "    \n",
    "    for i in range(rows*cols):\n",
    "        ax = fig.add_subplot(rows, cols, i+1)\n",
    "        ax.imshow(weights[i].view(5, 10).cpu().numpy(), cmap = 'bone')\n",
    "        #plt.title(str(train_target[i]))\n",
    "        ax.axis('off')\n",
    "        \n",
    "def get_pca(data, data_test=None, n_components = 2):\n",
    "    pca = decomposition.PCA()\n",
    "    \n",
    "    pca.n_components = n_components\n",
    "    pca_data = pca.fit_transform(data)\n",
    "    \n",
    "    if data_test is not None: \n",
    "        pca_data_test = pca.transform(data_test)\n",
    "        return pca_data, pca_data_test \n",
    "    \n",
    "    return pca_data\n",
    "\n",
    "\n",
    "def get_tsne(data, data_test = None, n_components = 2, n_curves = None):\n",
    "    if n_curves is not None:\n",
    "        data = data[:n_curves]\n",
    "    tsne = manifold.TSNE(n_components = n_components, random_state = 0)\n",
    "    tsne_data = tsne.fit_transform(data)\n",
    "    \n",
    "    if data_test is not None: \n",
    "        tsne_data_test = tsne.fit_transform(data_test)\n",
    "        return tsne_data, tsne_data_test  \n",
    "    \n",
    "    return tsne_data\n",
    "\n",
    "def get_representations(model, iterator, device):\n",
    "\n",
    "    model.eval()\n",
    "    outputs = []\n",
    "    intermediates = []\n",
    "    intermediates2 = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (x, y) in iterator:\n",
    "            x = x.to(device)\n",
    "            y_pred, h2, h1 = net(x)\n",
    "            outputs.append(y_pred.cpu())\n",
    "            intermediates.append(h1.cpu())\n",
    "            intermediates2.append(h2.cpu())\n",
    "            labels.append(y)\n",
    "        \n",
    "    outputs = torch.cat(outputs, dim = 0)\n",
    "    intermediates = torch.cat(intermediates, dim = 0)\n",
    "    intermediates2 = torch.cat(intermediates2, dim = 0)\n",
    "    labels = torch.cat(labels, dim = 0)\n",
    "\n",
    "    return outputs, intermediates, intermediates2, labels\n",
    "\n",
    "def plot_representations(data, labels, ax, n_curves = None):\n",
    "    if n_curves is not None:\n",
    "        data = data[:n_curves]\n",
    "        labels = labels[:n_curves]\n",
    "    #fig = plt.figure(figsize = (10, 10))\n",
    "    #ax = fig.add_subplot(111)\n",
    "    scatter = ax.scatter(data[:, 0], data[:, 1], c = labels, alpha =0.5)\n",
    "    handles, labels = scatter.legend_elements()\n",
    "    legend = ax.legend(handles = handles, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 1\n",
    "fileTrain = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Train_rrlyr-'+str(number)+'.csv'\n",
    "fileTest = '/home/franciscoperez/Documents/GitHub/data/BIASEDFATS/Test_rrlyr-'+str(number)+'.csv'\n",
    "train_dataset = pd.read_csv(fileTrain, index_col ='Unnamed: 0')\n",
    "test_dataset = pd.read_csv(fileTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq1_harmonics_rel_phase_0\n",
      "Freq2_harmonics_rel_phase_0\n",
      "Freq3_harmonics_rel_phase_0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    train_dataset =  train_dataset.drop(['Pred', 'Pred2', 'h', 'e', 'u','ID'], axis = 1)\n",
    "    for col in train_dataset.columns:\n",
    "        if col not in ['label']:\n",
    "            if train_dataset[col].var()==0:\n",
    "                print(col)\n",
    "                del train_dataset[col]\n",
    "    test_dataset = test_dataset[list(train_dataset.columns)]\n",
    "except:\n",
    "    print(col)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000#train_dataset.shape[0] \n",
    "epsilon = 0.005\n",
    "input_size = train_dataset.shape[1]-1\n",
    "hidden_size = 500\n",
    "hidden_size2 = 500\n",
    "num_classes = 2\n",
    "num_epochs = 200\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "learning_rate2 = 0.001\n",
    "regularization = False\n",
    "add_DR_based_data = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 61)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = train_dataset.sample(n)\n",
    "train_dataset.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28625, 61)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset['PeriodLS'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7806, 61)\n"
     ]
    }
   ],
   "source": [
    "label = train_dataset['label']\n",
    "del train_dataset['label']\n",
    "\n",
    "\n",
    "train_dataset_z=(train_dataset-train_dataset.mean())/train_dataset.std()\n",
    "z_scores = stats.zscore(train_dataset_z)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "train_dataset['label'] = label\n",
    "train_dataset = train_dataset[filtered_entries]\n",
    "print(train_dataset.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28625, 60)\n",
      "(28625, 61)\n"
     ]
    }
   ],
   "source": [
    "label = test_dataset['label']\n",
    "del test_dataset['label']\n",
    "\n",
    "print(test_dataset.shape)\n",
    "\n",
    "test_dataset_z=(test_dataset-test_dataset.mean())/test_dataset.std()\n",
    "z_scores = stats.zscore(test_dataset_z)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
    "test_dataset['label'] = label\n",
    "\n",
    "print(test_dataset.shape)\n",
    "\n",
    "test_dataset = test_dataset[filtered_entries]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = test_dataset[['PeriodLS', 'Amplitude', 'AndersonDarling', 'Autocor_length', 'Beyond1Std',\n",
    "       'CAR_mean', 'CAR_sigma', 'CAR_tau', 'Con', 'Eta_e',\n",
    "       'FluxPercentileRatioMid20', 'FluxPercentileRatioMid35',\n",
    "       'FluxPercentileRatioMid50', 'FluxPercentileRatioMid65',\n",
    "       'FluxPercentileRatioMid80', 'Freq1_harmonics_amplitude_0',\n",
    "       'Freq1_harmonics_amplitude_1', 'Freq1_harmonics_amplitude_2',\n",
    "       'Freq1_harmonics_amplitude_3', 'Freq1_harmonics_rel_phase_1',\n",
    "       'Freq1_harmonics_rel_phase_2', 'Freq1_harmonics_rel_phase_3',\n",
    "       'Freq2_harmonics_amplitude_0', 'Freq2_harmonics_amplitude_1',\n",
    "       'Freq2_harmonics_amplitude_2', 'Freq2_harmonics_amplitude_3',\n",
    "       'Freq2_harmonics_rel_phase_1', 'Freq2_harmonics_rel_phase_2',\n",
    "       'Freq2_harmonics_rel_phase_3', 'Freq3_harmonics_amplitude_0',\n",
    "       'Freq3_harmonics_amplitude_1', 'Freq3_harmonics_amplitude_2',\n",
    "       'Freq3_harmonics_amplitude_3', 'Freq3_harmonics_rel_phase_1',\n",
    "       'Freq3_harmonics_rel_phase_2', 'Freq3_harmonics_rel_phase_3', 'Gskew',\n",
    "       'LinearTrend', 'MaxSlope', 'Mean', 'Meanvariance', 'MedianAbsDev',\n",
    "       'MedianBRP', 'PairSlopeTrend', 'PercentAmplitude',\n",
    "       'PercentDifferenceFluxPercentile', 'Period_fit', 'Psi_CS',\n",
    "       'Psi_eta', 'Q31', 'Rcs', 'Skew', 'SlottedA_length', 'SmallKurtosis',\n",
    "       'Std', 'StetsonK', 'StetsonK_AC', 'StructureFunction_index_21',\n",
    "       'StructureFunction_index_31', 'StructureFunction_index_32', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset[['PeriodLS', 'Amplitude', 'AndersonDarling', 'Autocor_length', 'Beyond1Std',\n",
    "       'CAR_mean', 'CAR_sigma', 'CAR_tau', 'Con', 'Eta_e',\n",
    "       'FluxPercentileRatioMid20', 'FluxPercentileRatioMid35',\n",
    "       'FluxPercentileRatioMid50', 'FluxPercentileRatioMid65',\n",
    "       'FluxPercentileRatioMid80', 'Freq1_harmonics_amplitude_0',\n",
    "       'Freq1_harmonics_amplitude_1', 'Freq1_harmonics_amplitude_2',\n",
    "       'Freq1_harmonics_amplitude_3', 'Freq1_harmonics_rel_phase_1',\n",
    "       'Freq1_harmonics_rel_phase_2', 'Freq1_harmonics_rel_phase_3',\n",
    "       'Freq2_harmonics_amplitude_0', 'Freq2_harmonics_amplitude_1',\n",
    "       'Freq2_harmonics_amplitude_2', 'Freq2_harmonics_amplitude_3',\n",
    "       'Freq2_harmonics_rel_phase_1', 'Freq2_harmonics_rel_phase_2',\n",
    "       'Freq2_harmonics_rel_phase_3', 'Freq3_harmonics_amplitude_0',\n",
    "       'Freq3_harmonics_amplitude_1', 'Freq3_harmonics_amplitude_2',\n",
    "       'Freq3_harmonics_amplitude_3', 'Freq3_harmonics_rel_phase_1',\n",
    "       'Freq3_harmonics_rel_phase_2', 'Freq3_harmonics_rel_phase_3', 'Gskew',\n",
    "       'LinearTrend', 'MaxSlope', 'Mean', 'Meanvariance', 'MedianAbsDev',\n",
    "       'MedianBRP', 'PairSlopeTrend', 'PercentAmplitude',\n",
    "       'PercentDifferenceFluxPercentile', 'Period_fit', 'Psi_CS',\n",
    "       'Psi_eta', 'Q31', 'Rcs', 'Skew', 'SlottedA_length', 'SmallKurtosis',\n",
    "       'Std', 'StetsonK', 'StetsonK_AC', 'StructureFunction_index_21',\n",
    "       'StructureFunction_index_31', 'StructureFunction_index_32', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_pred = test_dataset.copy()\n",
    "train_dataset_pred = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PeriodLS</th>\n",
       "      <th>Amplitude</th>\n",
       "      <th>AndersonDarling</th>\n",
       "      <th>Autocor_length</th>\n",
       "      <th>Beyond1Std</th>\n",
       "      <th>CAR_mean</th>\n",
       "      <th>CAR_sigma</th>\n",
       "      <th>CAR_tau</th>\n",
       "      <th>Con</th>\n",
       "      <th>Eta_e</th>\n",
       "      <th>...</th>\n",
       "      <th>Skew</th>\n",
       "      <th>SlottedA_length</th>\n",
       "      <th>SmallKurtosis</th>\n",
       "      <th>Std</th>\n",
       "      <th>StetsonK</th>\n",
       "      <th>StetsonK_AC</th>\n",
       "      <th>StructureFunction_index_21</th>\n",
       "      <th>StructureFunction_index_31</th>\n",
       "      <th>StructureFunction_index_32</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>391262</th>\n",
       "      <td>74.277705</td>\n",
       "      <td>0.15100</td>\n",
       "      <td>0.001250</td>\n",
       "      <td>10</td>\n",
       "      <td>0.346667</td>\n",
       "      <td>22.791959</td>\n",
       "      <td>0.537868</td>\n",
       "      <td>0.619429</td>\n",
       "      <td>0.02349</td>\n",
       "      <td>14.104876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194838</td>\n",
       "      <td>17.79498</td>\n",
       "      <td>-0.488208</td>\n",
       "      <td>0.078843</td>\n",
       "      <td>0.814231</td>\n",
       "      <td>0.784707</td>\n",
       "      <td>1.801910</td>\n",
       "      <td>2.535099</td>\n",
       "      <td>1.448028</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230291</th>\n",
       "      <td>1020.470095</td>\n",
       "      <td>0.01750</td>\n",
       "      <td>0.115532</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363333</td>\n",
       "      <td>274.744191</td>\n",
       "      <td>0.211135</td>\n",
       "      <td>0.053812</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>5.686240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.133719</td>\n",
       "      <td>5.69361</td>\n",
       "      <td>0.111186</td>\n",
       "      <td>0.009347</td>\n",
       "      <td>0.793480</td>\n",
       "      <td>0.745731</td>\n",
       "      <td>1.620815</td>\n",
       "      <td>2.129265</td>\n",
       "      <td>1.607298</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26148</th>\n",
       "      <td>0.486126</td>\n",
       "      <td>0.02575</td>\n",
       "      <td>0.536316</td>\n",
       "      <td>2</td>\n",
       "      <td>0.299065</td>\n",
       "      <td>39.534998</td>\n",
       "      <td>0.553698</td>\n",
       "      <td>0.389701</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>6307.485427</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058798</td>\n",
       "      <td>4.92600</td>\n",
       "      <td>-0.091798</td>\n",
       "      <td>0.012686</td>\n",
       "      <td>0.791180</td>\n",
       "      <td>0.738202</td>\n",
       "      <td>1.453116</td>\n",
       "      <td>1.593353</td>\n",
       "      <td>1.206530</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34003</th>\n",
       "      <td>921.153420</td>\n",
       "      <td>0.01825</td>\n",
       "      <td>0.253883</td>\n",
       "      <td>5</td>\n",
       "      <td>0.283186</td>\n",
       "      <td>206.752429</td>\n",
       "      <td>0.049652</td>\n",
       "      <td>0.068576</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>8.364235</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.502323</td>\n",
       "      <td>1.18029</td>\n",
       "      <td>1.457469</td>\n",
       "      <td>0.010113</td>\n",
       "      <td>0.777735</td>\n",
       "      <td>0.741196</td>\n",
       "      <td>1.317766</td>\n",
       "      <td>1.621916</td>\n",
       "      <td>1.385394</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189282</th>\n",
       "      <td>1.139047</td>\n",
       "      <td>0.45500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>18.179650</td>\n",
       "      <td>0.189635</td>\n",
       "      <td>1.087083</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>17.117858</td>\n",
       "      <td>...</td>\n",
       "      <td>1.129390</td>\n",
       "      <td>1.89933</td>\n",
       "      <td>2.278362</td>\n",
       "      <td>0.219409</td>\n",
       "      <td>0.742077</td>\n",
       "      <td>0.726959</td>\n",
       "      <td>2.211178</td>\n",
       "      <td>3.532070</td>\n",
       "      <td>1.632040</td>\n",
       "      <td>ClassB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           PeriodLS  Amplitude  AndersonDarling  Autocor_length  Beyond1Std  \\\n",
       "391262    74.277705    0.15100         0.001250              10    0.346667   \n",
       "230291  1020.470095    0.01750         0.115532               2    0.363333   \n",
       "26148      0.486126    0.02575         0.536316               2    0.299065   \n",
       "34003    921.153420    0.01825         0.253883               5    0.283186   \n",
       "189282     1.139047    0.45500         0.000000               1    0.220000   \n",
       "\n",
       "          CAR_mean  CAR_sigma   CAR_tau      Con        Eta_e  ...      Skew  \\\n",
       "391262   22.791959   0.537868  0.619429  0.02349    14.104876  ...  0.194838   \n",
       "230291  274.744191   0.211135  0.053812  0.00000     5.686240  ...  0.133719   \n",
       "26148    39.534998   0.553698  0.389701  0.00000  6307.485427  ... -0.058798   \n",
       "34003   206.752429   0.049652  0.068576  0.00000     8.364235  ... -0.502323   \n",
       "189282   18.179650   0.189635  1.087083  0.00000    17.117858  ...  1.129390   \n",
       "\n",
       "        SlottedA_length  SmallKurtosis       Std  StetsonK  StetsonK_AC  \\\n",
       "391262         17.79498      -0.488208  0.078843  0.814231     0.784707   \n",
       "230291          5.69361       0.111186  0.009347  0.793480     0.745731   \n",
       "26148           4.92600      -0.091798  0.012686  0.791180     0.738202   \n",
       "34003           1.18029       1.457469  0.010113  0.777735     0.741196   \n",
       "189282          1.89933       2.278362  0.219409  0.742077     0.726959   \n",
       "\n",
       "        StructureFunction_index_21  StructureFunction_index_31  \\\n",
       "391262                    1.801910                    2.535099   \n",
       "230291                    1.620815                    2.129265   \n",
       "26148                     1.453116                    1.593353   \n",
       "34003                     1.317766                    1.621916   \n",
       "189282                    2.211178                    3.532070   \n",
       "\n",
       "        StructureFunction_index_32   label  \n",
       "391262                    1.448028  ClassB  \n",
       "230291                    1.607298  ClassB  \n",
       "26148                     1.206530  ClassB  \n",
       "34003                     1.385394  ClassB  \n",
       "189282                    1.632040  ClassB  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4c8b908a10>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset[train_dataset.label=='ClassA'].Amplitude.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 5000\n",
    "samples1 = samples*2\n",
    "number_columns = train_dataset.shape[1]\n",
    "option = 2\n",
    "\n",
    "\n",
    "data_prior = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns)\n",
    "\n",
    "if add_DR_based_data:\n",
    "    #option 1\n",
    "    if option == 1:\n",
    "        for i in range(samples1):\n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']= (np.random.uniform(0.2-epsilon,1.0+epsilon))#-minimum_period)/(maximum_period-minimum_period)\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "    \n",
    "    if option==2:\n",
    "        #option 2\n",
    "        for i in range(samples):\n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']=(np.random.uniform(0.2-epsilon,0.2))#-minimum_period)/(maximum_period-minimum_period)\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']=(np.random.uniform(1.0,1.0+epsilon))\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        \n",
    "    #option 3\n",
    "    if option==3:\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) #pd.DataFrame([train_dataset.sample(1000).mean()]).T\n",
    "            new_data['PeriodLS']= 1.0\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "\n",
    "        for i in range(samples):    \n",
    "            new_data = pd.DataFrame(0, index=np.arange(1), columns=train_dataset.columns) \n",
    "            new_data.columns = train_dataset.columns\n",
    "            new_data['PeriodLS']= 0.2\n",
    "            new_data['label'] = 'Noise'\n",
    "            frames = [data_prior, new_data]\n",
    "            data_prior = pd.concat(frames, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prior.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior, val_dataset_prior = train_test_split(data_prior, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "\n",
    "train_target_prior = torch.tensor(train_dataset_prior['label'].values.astype(np.float32))\n",
    "train_prior = torch.tensor(train_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_prior = f.normalize(train_prior)\n",
    "train_tensor_prior = data_utils.TensorDataset(train_prior, train_target_prior) \n",
    "train_loader_prior = data_utils.DataLoader(dataset = train_tensor_prior, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "val_target_prior = torch.tensor(val_dataset_prior['label'].values.astype(np.float32))\n",
    "val_prior = torch.tensor(val_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val_prior = f.normalize(val_prior)\n",
    "val_tensor_prior = data_utils.TensorDataset(val_prior, val_target_prior) \n",
    "val_loader_prior = data_utils.DataLoader(dataset = val_tensor_prior, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases using DR 1: Period $ \\in [0.2,1.0]$ days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassA', '1')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassB', '0')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('Noise', '0.5')\n",
    "train_target = torch.tensor(train_dataset['label'].values.astype(np.float32))\n",
    "train = torch.tensor(train_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train = f.normalize(train)\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "train_target_pred = torch.tensor(train_dataset_pred['label'].values.astype(np.float32))\n",
    "train_pred = torch.tensor(train_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_pred = f.normalize(train_pred)\n",
    "train_tensor_pred = data_utils.TensorDataset(train_pred, train_target_pred) \n",
    "train_loader_pred = data_utils.DataLoader(dataset = train_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassA', '1')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassB', '0')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('Noise', '0.5')\n",
    "val_target = torch.tensor(val_dataset['label'].values.astype(np.float32))\n",
    "val = torch.tensor(val_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val = f.normalize(val)\n",
    "val_tensor = data_utils.TensorDataset(val, val_target) \n",
    "val_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassA', '1')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassB', '0')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('Noise', '0.5')\n",
    "test_target = torch.tensor(test_dataset['label'].values.astype(np.float32))\n",
    "test = torch.tensor(test_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test = f.normalize(test)\n",
    "test_tensor = data_utils.TensorDataset(test, test_target) \n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "test_target_pred = torch.tensor(test_dataset_pred['label'].values.astype(np.float32))\n",
    "test_pred = torch.tensor(test_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test_pred = f.normalize(test_pred)\n",
    "test_tensor_pred = data_utils.TensorDataset(test_pred, test_target_pred) \n",
    "test_loader_pred = data_utils.DataLoader(dataset = test_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        #self.relu = nn.ReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size2)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes) \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.fc1(x)\n",
    "        #out = self.batchnorm1(out)\n",
    "        #out = self.relu(out)\n",
    "        out = self.relu(out1)\n",
    "        out2 = self.fc2(out)\n",
    "        #out = self.batchnorm1(out)\n",
    "        out = self.relu2(out)\n",
    "        out3 = self.fc3(out)\n",
    "        #x = self.dropout(x)\n",
    "        #out = self.sigmoid(out)\n",
    "        return out3, out2, out1\n",
    "  \n",
    "#net_prior = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "#net_prior.cuda()\n",
    "\n",
    "net = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "\n",
    "net.cuda()\n",
    "#net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_l1 = list(net.fc1.parameters())\n",
    "param_l2 = list(net.fc2.parameters())\n",
    "param_l3 = list(net.fc3.parameters())\n",
    "#print(len(param))\n",
    "print(len(param_l1[0]))\n",
    "print(len(param_l1[:][0]))\n",
    "#print(len(param[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "optimizer_prior = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.9) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): \n",
    "    return x.exp() / (x.exp().sum(-1)).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(output, labels, weigths=None, weigths_prior=None):\n",
    "    regularization_loss = 0\n",
    "    lambda_1 = 0.00001\n",
    "    if weigths is not None: \n",
    "        regularization_loss += fast_cdist(weigths, weigths_prior)\n",
    "            #print(regularization_loss)\n",
    "        loss = criterion(outputs, labels) + lambda_1*regularization_loss #nn.L1Loss()(weigths, weigths)\n",
    "    else:\n",
    "        loss = criterion(outputs, labels)\n",
    "        #print(loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss_auxiliar(output, lambda_pen=1.0):\n",
    "    #print(softmax(output)[:,1])\n",
    "    expected_loss = softmax(output)[:,1].mean()\n",
    "    #print(expected_loss)\n",
    "    threshold = torch.tensor(0.5).cuda()\n",
    "    aux_loss = torch.tensor(expected_loss-threshold, requires_grad=True).cuda()\n",
    "    #print(aux_loss**2)\n",
    "    return torch.tensor(lambda_pen*aux_loss**2, requires_grad=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_cdist(x1, x2):\n",
    "    res=f.mse_loss(x1, x2, size_average=False)\n",
    "    #res=f.l1_loss(x1, x2, size_average=False)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "aux_loss_activated = True\n",
    "# Train the Model   \n",
    "loss_prior = torch.tensor(0)\n",
    "hist_train = []\n",
    "hist_val = []\n",
    "aux_loss_behaviour = []\n",
    "num_epochs_prior = 2000\n",
    "\n",
    "\n",
    "EPS = 1e-3\n",
    "locked_masks = {n: torch.abs(w) < EPS for n, w in net.named_parameters() if n.endswith('weight')}\n",
    "\n",
    "EPS = 1e-6\n",
    "locked_masks2 = {n: torch.abs(w) < EPS for n, w in net.named_parameters() if n.endswith('weight')}\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs_prior):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    epoch_loss_prior = 0.0    \n",
    "    running_loss_prior = 0.0\n",
    "    \n",
    "    for item1, item2 in zip(train_loader, cycle(train_loader_prior)):\n",
    "        star_prior, labels_prior = item2\n",
    "        star, labels = item1\n",
    "        \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  \n",
    "        outputs, _, _ = net(star)\n",
    "        loss = criterion(outputs, labels.long())      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if aux_loss_activated:\n",
    "            star_prior = Variable(star_prior.view(-1, input_size)).cuda()\n",
    "            labels_prior = Variable(labels_prior).cuda()\n",
    "            optimizer_prior.zero_grad()  # zero the gradient buffer\n",
    "            outputs_prior, _, _ = net(star_prior)\n",
    "            #print(\"-------------------before----------------------\")\n",
    "            aux_loss_behaviour.append(loss_prior.item())\n",
    "            loss_prior = criterion(outputs_prior, labels_prior.long()) #custom_loss_auxiliar(outputs_prior)      \n",
    "            #print(\"-------------------later-----------------------\")\n",
    "            aux_loss_behaviour.append(loss_prior.item())\n",
    "            loss_prior.backward()\n",
    "            \n",
    "            for n, w in net.named_parameters():                                                                                                                                                                           \n",
    "                if w.grad is not None and n in locked_masks2:                                                                                                                                                                                   \n",
    "                    w.grad[locked_masks2[n]] = 0 \n",
    "                \n",
    "                \n",
    "            optimizer_prior.step()\n",
    "            epoch_loss_prior += outputs_prior.shape[0] * loss_prior.item()      \n",
    "            running_loss_prior += loss_prior.item()\n",
    "        \n",
    "    hist_train.append(running_loss/len(train_loader))    \n",
    "    print('training:', 'epoch: ', str(epoch+1),' loss: ', str(running_loss/len(train_loader)))\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (star, labels) in enumerate(val_loader):  \n",
    "        \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  \n",
    "        outputs, _, _ = net(star)\n",
    "        loss = criterion(outputs, labels.long())      \n",
    "        loss.backward()\n",
    "        \n",
    "        for n, w in net.named_parameters():                                                                                                                                                                           \n",
    "            if w.grad is not None and n in locked_masks:                                                                                                                                                                                   \n",
    "                w.grad[locked_masks[n]] = 0 \n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('validating:', 'epoch: ', str(epoch+1),' loss: ', str(running_loss / len(val_loader)))\n",
    "    hist_val.append(running_loss / len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aux_loss_behaviour[0:100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in test_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on test objects: %d %%' % (100 * correct / total))\n",
    "acc_testing = 100 *correct / total\n",
    "print(np.asarray(acc_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in train_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on train objects: %d %%' % (100 * correct / total))\n",
    "acc_training = 100 *correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_val, label ='validation')\n",
    "plt.plot(hist_train, label ='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.savefig('images/'+str(samples)+'_'+str(epsilon)+'_'+str(n)+\"_\"+str(hidden_size)+\"_Loss_Training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open(\"size_MLP_noise.csv\", \"a\")\n",
    "csv_file.write(str(np.asarray(acc_testing))+\",\"+str(np.asarray(acc_training))+\",\"+str(samples)+\",\"+str(epsilon)+\",\"+str(n)+\",\"+str(hidden_size)+\"\\n\")\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, intermediates, intermediates2, labels = get_representations(net, train_loader, device)\n",
    "outputs_test, intermediates_test, intermediates2_test, labels_test = get_representations(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pca_data, intermediate_pca_data_test = get_pca(intermediates, data_test=intermediates_test)\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "\n",
    "plot_representations(intermediate_pca_data, labels, axs[0, 0])\n",
    "plot_representations(intermediate_pca_data_test, labels_test, axs[1, 0])\n",
    "\n",
    "intermediate2_pca_data, intermediate2_pca_data_test = get_pca(intermediates2, data_test=intermediates2_test)\n",
    "plot_representations(intermediate2_pca_data, labels, axs[0, 1])\n",
    "plot_representations(intermediate2_pca_data_test, labels_test, axs[1, 1])\n",
    "\n",
    "output_pca_data, output_pca_data_test = get_pca(outputs, data_test=outputs_test)\n",
    "plot_representations(output_pca_data, labels, axs[0, 2])\n",
    "plot_representations(output_pca_data_test, labels_test, axs[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CURVES = 25000\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "intermediate_tsne_data, intermediate_tsne_data_test = get_tsne(intermediates, data_test= intermediates_test, n_curves = N_CURVES)\n",
    "plot_representations(intermediate_tsne_data, labels, axs[0, 0],  n_curves = N_CURVES)\n",
    "plot_representations(intermediate_tsne_data_test, labels_test, axs[1, 0], n_curves = N_CURVES)\n",
    "\n",
    "intermediate2_tsne_data, intermediate2_tsne_data_test = get_tsne(intermediates2, data_test=intermediates2_test, n_curves = N_CURVES)\n",
    "plot_representations(intermediate2_tsne_data, labels, axs[0, 1], n_curves = N_CURVES)\n",
    "plot_representations(intermediate2_tsne_data_test, labels_test, axs[1, 1], n_curves = N_CURVES)\n",
    "\n",
    "output_tsne_data, output2_tsne_data_test = get_tsne(outputs, data_test=outputs_test, n_curves = N_CURVES)\n",
    "plot_representations(output_tsne_data, labels, axs[0, 2], n_curves = N_CURVES)\n",
    "plot_representations(output2_tsne_data_test, labels_test, axs[1, 2], n_curves = N_CURVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\n",
    "curves, labels, probs_train = get_predictions(net, train_loader_pred, device)\n",
    "#pred_labels = torch.argmax(probs_train, 1)\n",
    "pred_labels = probs_train.argmax(1, keepdim = True)\n",
    "\n",
    "plot_confusion_matrix(np.round(labels), pred_labels, ax1)\n",
    "curves, labels, probs_test = get_predictions(net, test_loader_pred, device)\n",
    "pred_labels = probs_test.argmax(1, keepdim = True)\n",
    "\n",
    "#pred_labels = torch.argmax(probs_test, 1)\n",
    "plot_confusion_matrix(np.round(labels), pred_labels, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_tensor))\n",
    "print(len(train_tensor_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves, labels, probs_train_sample = get_predictions(net, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "ax1.hist(probs_train[:,0], color='black')\n",
    "ax1.set_xlabel('training set')\n",
    "ax2.hist(probs_train_sample[:,0], color='black')\n",
    "ax2.set_xlabel('training set + samples')\n",
    "ax3.hist(probs_test[:,0], color='black')\n",
    "ax3.set_xlabel('testing set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(softmax(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WEIGHTS = 25\n",
    "weights = net.fc2.weight.data\n",
    "plot_weights(weights, N_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = net.fc1.weight.data\n",
    "w1 = weights1.cpu().numpy().reshape(-1,1)\n",
    "weights2 = net.fc2.weight.data\n",
    "w2 = weights2.cpu().numpy().reshape(-1,1)\n",
    "weights3 = net.fc3.weight.data\n",
    "w3 = weights3.cpu().numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "\n",
    "ax1.hist(w1, color='black')\n",
    "ax1.set_xlabel('Layer 1')\n",
    "ax2.hist(w2, color='black')\n",
    "ax2.set_xlabel('Layer 2')\n",
    "ax3.hist(w3, color='black')\n",
    "ax3.set_xlabel('Layer 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.functional as f \n",
    "from torch.autograd import Variable\n",
    "torch.backends.cudnn.deterministic = True\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "import random \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import decomposition\n",
    "from sklearn import manifold\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import utilities as ut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freq1_harmonics_rel_phase_0\n",
      "Freq2_harmonics_rel_phase_0\n",
      "Freq3_harmonics_rel_phase_0\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = ut.load_files(dataset=1)\n",
    "n = 5000#train_dataset.shape[0] \n",
    "epsilon = 0.1\n",
    "input_size = train_dataset.shape[1]-1\n",
    "hidden_size = 500\n",
    "hidden_size2 = 500\n",
    "num_classes = 2\n",
    "num_epochs = 200\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "learning_rate2 = 0.001\n",
    "regularization = False\n",
    "add_DR_based_data = True\n",
    "train_dataset = train_dataset.sample(n)\n",
    "samples = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3880, 61)\n",
      "(28625, 60)\n",
      "(28625, 61)\n"
     ]
    }
   ],
   "source": [
    "train_dataset, test_dataset = ut.delete_outliers(train_dataset, test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ut.sort_columns(train_dataset)\n",
    "test_dataset = ut.sort_columns(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_pred = test_dataset.copy()\n",
    "train_dataset_pred = train_dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'epsilon' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-60b2a6c6f3bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/GitHub/vsbms_multiple_classes/vsbms_multiple_classes/utilities.py\u001b[0m in \u001b[0;36mgenerate_samples\u001b[0;34m(samples, train_dataset, option)\u001b[0m\n\u001b[1;32m    209\u001b[0m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'PeriodLS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#-minimum_period)/(maximum_period-minimum_period)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m             \u001b[0mnew_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Noise'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m             \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdata_prior\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'epsilon' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_prior = ut.generate_samples(samples, train_dataset, epsilon,  option = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior, val_dataset_prior = train_test_split(data_prior, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "train_dataset_prior['label'] = train_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "\n",
    "train_target_prior = torch.tensor(train_dataset_prior['label'].values.astype(np.float32))\n",
    "train_prior = torch.tensor(train_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_prior = f.normalize(train_prior)\n",
    "train_tensor_prior = data_utils.TensorDataset(train_prior, train_target_prior) \n",
    "train_loader_prior = data_utils.DataLoader(dataset = train_tensor_prior, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassA', '1')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('ClassB', '0')\n",
    "val_dataset_prior['label'] = val_dataset_prior['label'].str.replace('Noise', '0.5')\n",
    "val_target_prior = torch.tensor(val_dataset_prior['label'].values.astype(np.float32))\n",
    "val_prior = torch.tensor(val_dataset_prior.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val_prior = f.normalize(val_prior)\n",
    "val_tensor_prior = data_utils.TensorDataset(val_prior, val_target_prior) \n",
    "val_loader_prior = data_utils.DataLoader(dataset = val_tensor_prior, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cases using DR 1: Period $ \\in [0.2,1.0]$ days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassA', '1')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('ClassB', '0')\n",
    "train_dataset['label'] = train_dataset['label'].str.replace('Noise', '0.5')\n",
    "train_target = torch.tensor(train_dataset['label'].values.astype(np.float32))\n",
    "train = torch.tensor(train_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train = f.normalize(train)\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset = train_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "train_dataset_pred['label'] = train_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "train_target_pred = torch.tensor(train_dataset_pred['label'].values.astype(np.float32))\n",
    "train_pred = torch.tensor(train_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "train_pred = f.normalize(train_pred)\n",
    "train_tensor_pred = data_utils.TensorDataset(train_pred, train_target_pred) \n",
    "train_loader_pred = data_utils.DataLoader(dataset = train_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassA', '1')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('ClassB', '0')\n",
    "val_dataset['label'] = val_dataset['label'].str.replace('Noise', '0.5')\n",
    "val_target = torch.tensor(val_dataset['label'].values.astype(np.float32))\n",
    "val = torch.tensor(val_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "val = f.normalize(val)\n",
    "val_tensor = data_utils.TensorDataset(val, val_target) \n",
    "val_loader = data_utils.DataLoader(dataset = val_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassA', '1')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('ClassB', '0')\n",
    "test_dataset['label'] = test_dataset['label'].str.replace('Noise', '0.5')\n",
    "test_target = torch.tensor(test_dataset['label'].values.astype(np.float32))\n",
    "test = torch.tensor(test_dataset.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test = f.normalize(test)\n",
    "test_tensor = data_utils.TensorDataset(test, test_target) \n",
    "test_loader = data_utils.DataLoader(dataset = test_tensor, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassA', '1')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('ClassB', '0')\n",
    "test_dataset_pred['label'] = test_dataset_pred['label'].str.replace('Noise', '0.5')\n",
    "test_target_pred = torch.tensor(test_dataset_pred['label'].values.astype(np.float32))\n",
    "test_pred = torch.tensor(test_dataset_pred.drop('label', axis = 1).values.astype(np.float32)) \n",
    "test_pred = f.normalize(test_pred)\n",
    "test_tensor_pred = data_utils.TensorDataset(test_pred, test_target_pred) \n",
    "test_loader_pred = data_utils.DataLoader(dataset = test_tensor_pred, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,hidden_size2, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        #self.relu = nn.ReLU()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size2)  \n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes) \n",
    "        #self.sigmoid = nn.Sigmoid()\n",
    "        #self.dropout = nn.Dropout(p=0.1)\n",
    "        #self.batchnorm1 = nn.BatchNorm1d(hidden_size)\n",
    "        #self.batchnorm2 = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.fc1(x)\n",
    "        #out = self.batchnorm1(out)\n",
    "        #out = self.relu(out)\n",
    "        out = self.relu(out1)\n",
    "        out2 = self.fc2(out)\n",
    "        #out = self.batchnorm1(out)\n",
    "        out = self.relu2(out)\n",
    "        out3 = self.fc3(out)\n",
    "        #x = self.dropout(x)\n",
    "        #out = self.sigmoid(out)\n",
    "        return out3, out2, out1\n",
    "  \n",
    "#net_prior = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "#net_prior.cuda()\n",
    "\n",
    "net = Net(input_size, hidden_size, hidden_size2, num_classes)\n",
    "\n",
    "#use_cuda = torch.cuda.is_available()\n",
    "\n",
    "net.cuda()\n",
    "#net = nn.DataParallel(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_l1 = list(net.fc1.parameters())\n",
    "param_l2 = list(net.fc2.parameters())\n",
    "param_l3 = list(net.fc3.parameters())\n",
    "#print(len(param))\n",
    "print(len(param_l1[0]))\n",
    "print(len(param_l1[:][0]))\n",
    "#print(len(param[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  \n",
    "#criterion = nn.BCELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)  \n",
    "optimizer_prior = torch.optim.Adam(net.parameters(), lr=learning_rate)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import cycle\n",
    "aux_loss_activated = True\n",
    "# Train the Model   \n",
    "loss_prior = torch.tensor(0)\n",
    "hist_train = []\n",
    "hist_val = []\n",
    "aux_loss_behaviour = []\n",
    "num_epochs_prior = 2000\n",
    "\n",
    "\n",
    "EPS = 1e-3\n",
    "locked_masks = {n: torch.abs(w) < EPS for n, w in net.named_parameters() if n.endswith('weight')}\n",
    "\n",
    "EPS = 1e-6\n",
    "locked_masks2 = {n: torch.abs(w) < EPS for n, w in net.named_parameters() if n.endswith('weight')}\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs_prior):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    epoch_loss_prior = 0.0    \n",
    "    running_loss_prior = 0.0\n",
    "    \n",
    "    for item1, item2 in zip(train_loader, cycle(train_loader_prior)):\n",
    "        star_prior, labels_prior = item2\n",
    "        star, labels = item1\n",
    "        \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  \n",
    "        outputs, _, _ = net(star)\n",
    "        loss = criterion(outputs, labels.long())      \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        if aux_loss_activated:\n",
    "            star_prior = Variable(star_prior.view(-1, input_size)).cuda()\n",
    "            labels_prior = Variable(labels_prior).cuda()\n",
    "            optimizer_prior.zero_grad()  # zero the gradient buffer\n",
    "            outputs_prior, _, _ = net(star_prior)\n",
    "            #print(\"-------------------before----------------------\")\n",
    "            aux_loss_behaviour.append(loss_prior.item())\n",
    "            loss_prior = criterion(outputs_prior, labels_prior.long()) #custom_loss_auxiliar(outputs_prior)      \n",
    "            #print(\"-------------------later-----------------------\")\n",
    "            aux_loss_behaviour.append(loss_prior.item())\n",
    "            loss_prior.backward()\n",
    "            \n",
    "            for n, w in net.named_parameters():                                                                                                                                                                           \n",
    "                if w.grad is not None and n in locked_masks2:                                                                                                                                                                                   \n",
    "                    w.grad[locked_masks2[n]] = 0 \n",
    "                \n",
    "                \n",
    "            optimizer_prior.step()\n",
    "            epoch_loss_prior += outputs_prior.shape[0] * loss_prior.item()      \n",
    "            running_loss_prior += loss_prior.item()\n",
    "        \n",
    "    hist_train.append(running_loss/len(train_loader))    \n",
    "    print('training:', 'epoch: ', str(epoch+1),' loss: ', str(running_loss/len(train_loader)))\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i, (star, labels) in enumerate(val_loader):  \n",
    "        \n",
    "        star = Variable(star.view(-1, input_size)).cuda()\n",
    "        labels = Variable(labels).cuda()\n",
    "        optimizer.zero_grad()  \n",
    "        outputs, _, _ = net(star)\n",
    "        loss = criterion(outputs, labels.long())      \n",
    "        loss.backward()\n",
    "        \n",
    "        for n, w in net.named_parameters():                                                                                                                                                                           \n",
    "            if w.grad is not None and n in locked_masks:                                                                                                                                                                                   \n",
    "                w.grad[locked_masks[n]] = 0 \n",
    "        \n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print('validating:', 'epoch: ', str(epoch+1),' loss: ', str(running_loss / len(val_loader)))\n",
    "    hist_val.append(running_loss / len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(aux_loss_behaviour)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the Model\n",
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in test_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on test objects: %d %%' % (100 * correct / total))\n",
    "acc_testing = 100 *correct / total\n",
    "print(np.asarray(acc_testing))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for star, labels in train_loader:\n",
    "    images = Variable(star.view(-1, input_size)).cuda()\n",
    "    outputs, _, _ = net(images)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels.long()).sum()\n",
    "print('Accuracy of the network on train objects: %d %%' % (100 * correct / total))\n",
    "acc_training = 100 *correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Model\n",
    "torch.save(net.state_dict(), 'model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hist_val, label ='validation')\n",
    "plt.plot(hist_train, label ='train')\n",
    "plt.legend()\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.savefig('images/'+str(samples)+'_'+str(epsilon)+'_'+str(n)+\"_\"+str(hidden_size)+\"_Loss_Training.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = open(\"size_MLP_noise.csv\", \"a\")\n",
    "csv_file.write(str(np.asarray(acc_testing))+\",\"+str(np.asarray(acc_training))+\",\"+str(samples)+\",\"+str(epsilon)+\",\"+str(n)+\",\"+str(hidden_size)+\"\\n\")\n",
    "csv_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, intermediates, intermediates2, labels = ut.get_representations(net, train_loader, device)\n",
    "outputs_test, intermediates_test, intermediates2_test, labels_test = ut.get_representations(net, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_pca_data, intermediate_pca_data_test = ut.get_pca(intermediates, data_test=intermediates_test)\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "ut.plot_representations(intermediate_pca_data, labels, axs[0, 0])\n",
    "ut.plot_representations(intermediate_pca_data_test, labels_test, axs[1, 0])\n",
    "intermediate2_pca_data, intermediate2_pca_data_test = ut.get_pca(intermediates2, data_test=intermediates2_test)\n",
    "ut.plot_representations(intermediate2_pca_data, labels, axs[0, 1])\n",
    "ut.plot_representations(intermediate2_pca_data_test, labels_test, axs[1, 1])\n",
    "output_pca_data, output_pca_data_test = ut.get_pca(outputs, data_test=outputs_test)\n",
    "ut.plot_representations(output_pca_data, labels, axs[0, 2])\n",
    "ut.plot_representations(output_pca_data_test, labels_test, axs[1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CURVES = 25000\n",
    "\n",
    "fig, axs = plt.subplots(2, 3, figsize=(20,15))\n",
    "intermediate_tsne_data, intermediate_tsne_data_test = ut.get_tsne(intermediates, data_test= intermediates_test, n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate_tsne_data, labels, axs[0, 0],  n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate_tsne_data_test, labels_test, axs[1, 0], n_curves = N_CURVES)\n",
    "\n",
    "intermediate2_tsne_data, intermediate2_tsne_data_test = ut.get_tsne(intermediates2, data_test=intermediates2_test, n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate2_tsne_data, labels, axs[0, 1], n_curves = N_CURVES)\n",
    "ut.plot_representations(intermediate2_tsne_data_test, labels_test, axs[1, 1], n_curves = N_CURVES)\n",
    "\n",
    "output_tsne_data, output2_tsne_data_test = ut.get_tsne(outputs, data_test=outputs_test, n_curves = N_CURVES)\n",
    "ut.plot_representations(output_tsne_data, labels, axs[0, 2], n_curves = N_CURVES)\n",
    "ut.plot_representations(output2_tsne_data_test, labels_test, axs[1, 2], n_curves = N_CURVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15,15))\n",
    "curves, labels, probs_train = ut.get_predictions(net, train_loader_pred, device)\n",
    "pred_labels = probs_train.argmax(1, keepdim = True)\n",
    "ut.plot_confusion_matrix(np.round(labels), pred_labels, ax1)\n",
    "curves, labels, probs_test = ut.get_predictions(net, test_loader_pred, device)\n",
    "pred_labels = probs_test.argmax(1, keepdim = True)\n",
    "ut.plot_confusion_matrix(np.round(labels), pred_labels, ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_tensor))\n",
    "print(len(train_tensor_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curves, labels, probs_train_sample = ut.get_predictions(net, train_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "ax1.hist(probs_train[:,0], color='black')\n",
    "ax1.set_xlabel('training set')\n",
    "ax2.hist(probs_train_sample[:,0], color='black')\n",
    "ax2.set_xlabel('training set + samples')\n",
    "ax3.hist(probs_test[:,0], color='black')\n",
    "ax3.set_xlabel('testing set')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#N_WEIGHTS = 25\n",
    "#weights = net.fc2.weight.data\n",
    "#plot_weights(weights, N_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights1 = net.fc1.weight.data\n",
    "w1 = weights1.cpu().numpy().reshape(-1,1)\n",
    "weights2 = net.fc2.weight.data\n",
    "w2 = weights2.cpu().numpy().reshape(-1,1)\n",
    "weights3 = net.fc3.weight.data\n",
    "w3 = weights3.cpu().numpy().reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,15))\n",
    "ax1.hist(w1, color='black')\n",
    "ax1.set_xlabel('Layer 1')\n",
    "ax2.hist(w2, color='black')\n",
    "ax2.set_xlabel('Layer 2')\n",
    "ax3.hist(w3, color='black')\n",
    "ax3.set_xlabel('Layer 3')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
